This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  workflows/
    dq_push.yml
    dq_scheduled.yml
dq/
  app/
    assets/
      sample_dataset.zip
    app.py
    processing.py
    ui.py
  config/
    mappings.yml
    quality_gate.yml
    root_causes.yml
    rules.yml
    schema.yml
  regression/
    golden_dataset.zip
    golden_expected.json
  validate/
    __init__.py
    config.py
    constants.py
    ge.py
    metadata.py
    models.py
    output_persistence.py
    output_recurrence.py
    output.py
    paths.py
    penalty_utils.py
    query_utils.py
    rule_executor.py
    runner.py
    scoring.py
    stage_utils.py
  anomaly.py
  schema_drift.py
reports/
  templates/
    scorecard.html.jinja
scripts/
  publish_helpers/
    __init__.py
    constants.py
    copy.py
    history.py
    io.py
    render.py
  __init__.py
  get_run_id.py
  ingest_lib.py
  ingest_tables.py
  ingest.py
  profile_collector.py
  profile_table.py
  profile_tables.py
  profile_utils.py
  publish.py
  quality_gate.py
  regression.py
  score_helpers.py
  score.py
  validate_runner.py
tools/
  generate_synthetic.py
  synthetic_builder.py
  synthetic_cli.py
  synthetic_templates.py
.gitignore
Makefile
README.md
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="scripts/get_run_id.py">
#!/usr/bin/env python3
"""Print the run identifier stored in a staging run metadata file."""

from __future__ import annotations

import argparse
import json
from pathlib import Path


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Return the run_id stored under a stage directory."
    )
    parser.add_argument(
        "--stage-path",
        required=True,
        help="Path to staging data that contains run_metadata.json",
    )
    args = parser.parse_args()

    metadata_path = Path(args.stage_path) / "run_metadata.json"
    if not metadata_path.exists():
        raise SystemExit(f"Missing run metadata at {metadata_path}")

    metadata = json.loads(metadata_path.read_text())
    run_id = metadata.get("run_id")
    if not run_id:
        raise SystemExit(f"run_id missing in {metadata_path}")

    print(run_id)


if __name__ == "__main__":
    main()
</file>

<file path="Makefile">
PYTHON ?= python3
VENV ?= .venv
PYTHON_BIN = $(if $(wildcard $(VENV)/bin/python),$(VENV)/bin/python,$(PYTHON))
PIP_BIN = $(VENV)/bin/pip

DATASET ?= phase1
SEED ?= 42

RAW_DIR = data/raw/$(DATASET)/$(SEED)
STAGE_DIR = data/staging/$(DATASET)/$(SEED)
SCORE_JSON = reports/latest/score.json

SAMPLE_FORCE ?= 1
SAMPLE_FORCE_FLAG = $(if $(filter 1,$(SAMPLE_FORCE)),--force,)

INGEST_FORCE ?= 1
INGEST_FORCE_FLAG = $(if $(filter 1,$(INGEST_FORCE)),--force,)

RUN_ID_FROM_STAGE = $(PYTHON_BIN) scripts/get_run_id.py --stage-path "$(STAGE_DIR)"

.PHONY: setup sample ingest profile validate validate-only report run ensure-cmake

setup: ensure-cmake
	$(PYTHON) -m venv $(VENV)
	$(PIP_BIN) install --upgrade pip setuptools wheel
	PATH=$(VENV)/bin:$$PATH \
	PYARROW_BUNDLE_ARROW_CPP=1 \
	$(PIP_BIN) install -r requirements.txt

ensure-cmake:
	@if command -v cmake >/dev/null 2>&1; then \
		echo "cmake already installed at $$(command -v cmake)"; \
	elif command -v brew >/dev/null 2>&1; then \
		echo "cmake missing; installing via Homebrew..."; \
		brew install cmake; \
	else \
		echo "cmake is required to build pyarrow. Install it (e.g. 'brew install cmake' on macOS) and rerun 'make setup'."; \
		exit 1; \
	fi

sample:
	$(PYTHON_BIN) tools/generate_synthetic.py --dataset-name $(DATASET) --seed $(SEED) $(SAMPLE_FORCE_FLAG)

ingest:
	$(PYTHON_BIN) scripts/ingest.py --dataset-name $(DATASET) --seed $(SEED) $(INGEST_FORCE_FLAG)

profile:
	@if [ ! -d "$(STAGE_DIR)" ]; then \
		echo "Stage path $(STAGE_DIR) missing. Run `make ingest` first."; exit 1; \
	fi
	$(PYTHON_BIN) scripts/profile_tables.py --dataset-name $(DATASET) --seed $(SEED) --stage-path "$(STAGE_DIR)"

validate: ingest validate-only

validate-only:
	$(PYTHON_BIN) scripts/validate_runner.py --dataset-name $(DATASET) --seed $(SEED)
	@RUN_ID=$$($(RUN_ID_FROM_STAGE)); \
	$(PYTHON_BIN) scripts/score.py --run-id $$RUN_ID

report:
	@if [ ! -f "$(SCORE_JSON)" ]; then \
		echo "Score payload not found at $(SCORE_JSON). Run `make validate` first."; exit 1; \
	fi
	@RUN_ID=$$($(RUN_ID_FROM_STAGE)); \
	$(PYTHON_BIN) scripts/publish.py --run-id $$RUN_ID

run: sample ingest profile validate-only report
</file>

<file path=".github/workflows/dq_scheduled.yml">
name: DQ Scheduled Pipeline

on:
  schedule:
    # Run nightly at 04:00 UTC
    - cron: '0 4 * * *'
  workflow_dispatch: {}

permissions:
  contents: write

env:
  DATASET_NAME: phase1
  SEED: ${{ github.run_number }}
  SKIP_SAMPLE_DATA_GENERATION: false

jobs:
  scheduled-pipeline:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: requirements.txt

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Generate synthetic data (optional)
        if: env.SKIP_SAMPLE_DATA_GENERATION != 'true'
        run: |
          python tools/generate_synthetic.py \
            --dataset-name "$DATASET_NAME" \
            --seed "$SEED" \
            --force

      - name: Ingest data
        run: |
          python scripts/ingest.py \
            --dataset-name "$DATASET_NAME" \
            --seed "$SEED" \
            --force

      - name: Validate data
        run: |
          python scripts/validate_runner.py \
            --dataset-name "$DATASET_NAME" \
            --seed "$SEED"

      - name: Score run
        id: score
        shell: bash
        run: |
          STAGE_DIR="data/staging/$DATASET_NAME/$SEED"
          RUN_ID=$(STAGE_DIR="$STAGE_DIR" python - <<'PY'
from pathlib import Path
import json, os
metadata_path = Path(os.environ['STAGE_DIR']) / 'run_metadata.json'
print(json.loads(metadata_path.read_text())['run_id'])
PY
          )
          echo "run_id=$RUN_ID" >> "$GITHUB_OUTPUT"
          python scripts/score.py --run-id "$RUN_ID"

      - name: Build scorecard
        run: |
          python scripts/publish.py --run-id "${{ steps.score.outputs.run_id }}"

      - name: Enforce quality gate
        run: |
          python scripts/quality_gate.py --run-id "${{ steps.score.outputs.run_id }}"

      - name: Upload run artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dq-scheduled-run-${{ steps.score.outputs.run_id }}
          path: |
            reports/latest
            reports/runs/run_id=${{ steps.score.outputs.run_id }}
            data/marts/dq_check_results/run_id=${{ steps.score.outputs.run_id }}
            data/marts/dq_issue_log/run_id=${{ steps.score.outputs.run_id }}

      - name: Publish scorecard to GitHub Pages
        if: success()
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: reports/latest
          publish_branch: gh-pages
          user_name: github-actions[bot]
          user_email: github-actions[bot]@users.noreply.github.com
          commit_message: "chore: publish scheduled DQ scorecard run ${{ steps.score.outputs.run_id }}"
          allow_empty_commit: true
</file>

<file path="dq/app/app.py">
"""Entry point that delegates to the Streamlit UI module."""

from __future__ import annotations

from dq.app.ui import main


if __name__ == "__main__":
    main()
</file>

<file path="dq/app/processing.py">
"""Helpers for staging uploads and running the ingest/validation pipeline."""

from __future__ import annotations

import io
import os
import zipfile
from pathlib import Path
from typing import Iterable

import pandas as pd
from streamlit.runtime.uploaded_file_manager import UploadedFile

from scripts.ingest_lib import ingest_dataset
from scripts.ingest_tables import TABLE_SPECS
from dq.validate.models import ValidationSummary
from dq.validate.runner import ValidationRunner

REQUIRED_SOURCES = {spec["source"] for spec in TABLE_SPECS}


def _find_dataset_root(base: Path) -> Path | None:
    for dirpath, _, filenames in os.walk(base):
        if REQUIRED_SOURCES.issubset(set(filenames)):
            return Path(dirpath)
    return None


def _extract_archive(source: Path, target: Path) -> None:
    with zipfile.ZipFile(source, "r") as archive:
        archive.extractall(target)


def _stage_uploaded_files(uploads: Iterable[UploadedFile], target: Path) -> None:
    for upload in uploads:
        data = upload.read()
        if upload.name.lower().endswith(".zip"):
            with zipfile.ZipFile(io.BytesIO(data)) as archive:
                archive.extractall(target)
        else:
            (target / upload.name).write_bytes(data)


def prepare_raw_source(
    temp_root: Path, sample_archive: Path | None, uploads: list[UploadedFile] | None
) -> Path:
    raw_dir = temp_root / "raw"
    raw_dir.mkdir(parents=True, exist_ok=True)
    if sample_archive and sample_archive.exists():
        _extract_archive(sample_archive, raw_dir)
    if uploads:
        _stage_uploaded_files(uploads, raw_dir)
    dataset_root = _find_dataset_root(raw_dir)
    if dataset_root is None:
        raise ValueError(
            "Could not locate the required CSV exports. Provide the five DQSentry exports"
            " (districts, users, resources, events, newsletter) either zipped or uploaded together."
        )
    return dataset_root


def _build_cleaned_archive(stage_path: Path) -> bytes:
    parquet_dir = stage_path / "parquet"
    buffer = io.BytesIO()
    with zipfile.ZipFile(buffer, "w", compression=zipfile.ZIP_DEFLATED) as archive:
        for file_path in sorted(parquet_dir.glob("*.parquet")):
            archive.write(file_path, arcname=file_path.name)
    buffer.seek(0)
    return buffer.read()


def _build_exceptions_csv(issue_df: pd.DataFrame) -> bytes:
    columns = [
        "check_name",
        "table_name",
        "dimension",
        "issue_type",
        "severity",
        "affected_rows",
        "affected_pct",
        "probable_root_cause",
        "recommended_fix",
        "sample_bad_rows_json",
    ]
    available = [col for col in columns if col in issue_df.columns]
    target = issue_df[available].copy()
    return target.to_csv(index=False).encode("utf-8")


def run_validation_pipeline(
    dataset_name: str, raw_root: Path, stage_root: Path, run_id: str
) -> tuple[pd.DataFrame, bytes, bytes, bytes, ValidationSummary]:
    stage_root.mkdir(parents=True, exist_ok=True)
    ingest_paths = ingest_dataset(
        dataset_name=dataset_name,
        seed=0,
        force=True,
        raw_path=raw_root,
        stage_path=stage_root,
        run_id=run_id,
    )
    stage_path = Path(ingest_paths["stage_path"])
    runner = ValidationRunner(
        dataset_name,
        run_id,
        stage_path,
        Path(ingest_paths["db_path"]),
    )
    summary = runner.run()
    issue_df = pd.read_parquet(summary.issue_log_path)
    cleaned = _build_cleaned_archive(stage_path)
    issues_csv = issue_df.to_csv(index=False).encode("utf-8")
    exceptions_csv = _build_exceptions_csv(issue_df)
    return issue_df, cleaned, issues_csv, exceptions_csv, summary
</file>

<file path="dq/app/ui.py">
"""Streamlit UI surface for the DQSentry CSV validator."""

from __future__ import annotations

import uuid
from pathlib import Path
from tempfile import TemporaryDirectory
from typing import Optional

import pandas as pd
import streamlit as st
from streamlit.runtime.uploaded_file_manager import UploadedFile

from dq.app.processing import prepare_raw_source, run_validation_pipeline
from dq.validate.models import ValidationSummary

APP_ROOT = Path(__file__).resolve().parent
ASSETS_DIR = APP_ROOT / "assets"

SAMPLE_DATASETS = [
    {
        "id": "phase1-demo",
        "label": "Phase 1 synthetic demo (seed 42)",
        "description": "Same CSV exports that power the automated validations.",
        "archive": ASSETS_DIR / "sample_dataset.zip",
        "dataset_name": "phase1",
    }
]


def _render_results(
    summary: ValidationSummary,
    issue_df: pd.DataFrame,
    cleaned_bytes: bytes,
    issues_csv: bytes,
    exceptions_csv: bytes,
) -> None:
    st.success(
        f"Validation complete · run_id={summary.run_id} · dataset={summary.dataset_name}"
    )
    score_display = f"{summary.score:.1f}/100"
    cols = st.columns([1, 3, 1])
    cols[0].metric("Overall score", score_display)
    subscores = summary.subscores or {}
    if subscores:
        subscore_df = (
            pd.DataFrame(subscores.items(), columns=["dimension", "score"])
            .assign(score=lambda df: df["score"].round(2))
            .sort_values("score", ascending=False)
        )
        cols[1].bar_chart(subscore_df.set_index("dimension")["score"])
    cols[2].write(f"Run ID: {summary.run_id}")

    st.subheader("Issue log")
    if issue_df.empty:
        st.info("No issues detected. All checks passed cleanly.")
    else:
        display_columns = [
            col
            for col in [
                "table_name",
                "check_name",
                "dimension",
                "issue_type",
                "severity",
                "affected_rows",
                "affected_pct",
                "recommended_fix",
            ]
            if col in issue_df.columns
        ]
        st.dataframe(
            issue_df.sort_values(
                ["severity", "affected_pct"], ascending=[False, False]
            )[display_columns],
            use_container_width=True,
        )
    with st.expander("Download artifacts", expanded=True):
        st.download_button(
            "Download issues.csv",
            issues_csv,
            file_name="issues.csv",
            mime="text/csv",
        )
        st.download_button(
            "Download exceptions.csv",
            exceptions_csv,
            file_name="exceptions.csv",
            mime="text/csv",
        )
        st.download_button(
            "Download cleaned dataset (.zip)",
            cleaned_bytes,
            file_name="cleaned_dataset.zip",
            mime="application/zip",
        )


def main() -> None:
    st.set_page_config(page_title="DQSentry CSV validator", layout="wide")
    st.title("DQSentry CSV validator")
    st.write(
        "Upload your CSV exports or use the sample dataset to see the DQSentry "
        "validation score, issue log, and download-ready clean artifacts."
    )

    available_samples = [sample for sample in SAMPLE_DATASETS if sample["archive"].exists()]
    sample_options = ["(None)"] + [sample["label"] for sample in available_samples]
    default_index = 1 if len(sample_options) > 1 else 0
    selected_label = st.selectbox("Built-in sample dataset", sample_options, index=default_index)
    selected_sample = next(
        (sample for sample in available_samples if sample["label"] == selected_label), None
    )
    if selected_sample:
        st.caption(selected_sample["description"])

    uploaded_files: Optional[list[UploadedFile]] = st.file_uploader(
        "Upload CSV files or a ZIP archive",
        type=["csv", "zip"],
        accept_multiple_files=True,
    )

    if st.button("Run validations"):
        if not uploaded_files and not selected_sample:
            st.warning("Please upload data or select a sample dataset first.")
            return
        with st.spinner("Staging uploads and running validations..."):
            try:
                with TemporaryDirectory(prefix="dqsentry-") as tmpdir:
                    temp_root = Path(tmpdir)
                    raw_root = prepare_raw_source(
                        temp_root,
                        selected_sample["archive"] if selected_sample else None,
                        uploaded_files,
                    )
                    dataset_name = selected_sample["dataset_name"] if selected_sample else "streamlit-upload"
                    run_id = f"streamlit-{uuid.uuid4().hex[:8]}"
                    stage_root = temp_root / "stage"
                    issue_df, cleaned_bytes, issues_csv, exceptions_csv, summary = (
                        run_validation_pipeline(dataset_name, raw_root, stage_root, run_id)
                    )
                _render_results(summary, issue_df, cleaned_bytes, issues_csv, exceptions_csv)
            except ValueError as exc:
                st.error(str(exc))
            except Exception as exc:
                st.exception(exc)
</file>

<file path="dq/config/mappings.yml">
grade_bands:
  PreK-2:
    synonyms: ["prek-2", "k-2", "grades pre k to 2"]
  3-5:
    synonyms: ["grades 3-5", "third to fifth"]
  6-8:
    synonyms: ["grades 6-8", "middle school"]
  9-12:
    synonyms: ["grades 9-12", "high school"]
state_codes:
  AL: Alabama
  AK: Alaska
  AZ: Arizona
  CA: California
  NY: New York
district_overrides:
  "NYC": "NYC Public Schools"
</file>

<file path="dq/config/quality_gate.yml">
score_threshold: 90.0
critical_severity: 5
</file>

<file path="dq/config/root_causes.yml">
checks:
  users_district_id_not_null:
    probable_cause: "Upstream export omitted district join"
    recommended_fix: "Add district_id to export template and enforce ingestion join"
  users_email_not_null:
    probable_cause: "CRM export allows blank email when contact is incomplete"
    recommended_fix: "Add validation on source and require email before export"
  events_event_ts_not_null:
    probable_cause: "Event ingestion misses timestamp column from source system"
    recommended_fix: "Align ingestion mapping and backfill missing event_ts"
</file>

<file path="dq/config/schema.yml">
tables:
  users:
    description: "Core CRM data describing people with location and role metadata."
    primary_key: user_id
    columns:
      user_id:
        type: uuid
        nullable: false
      email:
        type: email
        nullable: false
      org_id:
        type: uuid
        nullable: false
      role:
        type: enum
        nullable: false
        allowed: [student, teacher, admin, district_admin, other]
      state:
        type: state_code
        nullable: false
      district_id:
        type: uuid
        nullable: true
  resources:
    description: "Catalog of learning objects, mapped to subject and grade intent."
    primary_key: resource_id
    columns:
      resource_id:
        type: uuid
        nullable: false
      type:
        type: enum
        nullable: false
        allowed: [lesson, assessment, activity, module, pathway]
      subject:
        type: string
        nullable: false
      grade_band:
        type: string
        nullable: false
  events:
    description: "Interaction log between users and resources."
    primary_key: event_id
    columns:
      event_id:
        type: uuid
        nullable: false
      user_id:
        type: uuid
        nullable: false
      resource_id:
        type: uuid
        nullable: false
      event_type:
        type: enum
        nullable: false
        allowed: [view, start, complete, submit, share]
      event_ts:
        type: timestamp
        nullable: false
  newsletter:
    description: "Marketing/crm newsletter status and engagement." 
    primary_key: email
    columns:
      email:
        type: email
        nullable: false
      subscribed_at:
        type: timestamp
        nullable: false
      opened_at:
        type: timestamp
        nullable: true
      clicked_at:
        type: timestamp
        nullable: true
  districts:
    description: "District reference data for alignment and reporting."
    primary_key: district_id
    columns:
      district_id:
        type: uuid
        nullable: false
      district_name:
        type: string
        nullable: false
      state:
        type: state_code
        nullable: false

required_fields:
  - users.user_id
  - users.email
  - events.event_id
  - events.user_id
  - events.resource_id
  - events.event_ts
  - newsletter.email
  - districts.district_id

column_expectations:
  users.state:
    pattern: '^[A-Z]{2}$'
  districts.state:
    pattern: '^[A-Z]{2}$'
</file>

<file path="dq/regression/golden_expected.json">
{
  "score": 95.52,
  "failed_checks": 8,
  "issue_counts": {
    "duplicate": 5,
    "missing": 2,
    "orphan": 1
  },
  "subscores": {
    "completeness": 97.7,
    "consistency": 100.0,
    "integrity": 94.0,
    "uniqueness": 87.02,
    "validity_accuracy": 100.0
  }
}
</file>

<file path="dq/validate/__init__.py">
"""Validation helpers for DQSentry."""
</file>

<file path="dq/validate/constants.py">
"""Shared validation constants for checks."""

from __future__ import annotations

ISSUE_TYPE_MAP = {
    "NULL_PERCENTAGE": "missing",
    "PATTERN": "invalid",
    "DATE_RANGE": "invalid",
    "NON_NEGATIVE_COUNTS": "invalid",
    "TIMESTAMP_ORDER": "inconsistency",
    "ENUM": "invalid",
    "UNIQUE_MAPPING": "inconsistency",
    "DUPLICATE_PERCENTAGE": "duplicate",
    "FK": "orphan",
}

SAMPLE_LIMIT = 5
</file>

<file path="dq/validate/metadata.py">
"""Collect metadata about staging tables."""

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List

import duckdb

from scripts.profile_utils import quote_ident


@dataclass
class StageMetadata:
    table_map: Dict[str, str]
    table_columns: Dict[str, List[str]]
    table_counts: Dict[str, int]


def collect_stage_metadata(con: duckdb.DuckDBPyConnection) -> StageMetadata:
    table_map: Dict[str, str] = {}
    table_columns: Dict[str, List[str]] = {}
    table_counts: Dict[str, int] = {}

    rows = con.execute(
        """
        SELECT table_name
        FROM information_schema.tables
        WHERE table_schema = 'main'
          AND table_type = 'BASE TABLE'
          AND table_name LIKE 'staging_%'
        ORDER BY table_name
        """
    ).fetchall()

    for (table,) in rows:
        logical = table.removeprefix("staging_")
        table_map[logical] = table
        columns = [
            row[0]
            for row in con.execute(
                f"""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_schema = 'main'
                  AND table_name = '{table}'
                ORDER BY ordinal_position
                """
            ).fetchall()
        ]
        table_columns[table] = columns
        table_counts[table] = con.execute(
            f"SELECT COUNT(*) FROM {quote_ident(table)}"
        ).fetchone()[0]

    return StageMetadata(
        table_map=table_map,
        table_columns=table_columns,
        table_counts=table_counts,
    )
</file>

<file path="dq/validate/output_persistence.py">
"""Persistence helpers for validation output artifacts."""

from __future__ import annotations

import json
from datetime import datetime
from pathlib import Path

import pandas as pd

from dq.validate.metadata import StageMetadata
from dq.validate.paths import DATA_MARTS_BASE


def persist_dataframe(df: pd.DataFrame, run_id: str, folder: str) -> Path:
    dest = DATA_MARTS_BASE / folder / f"run_id={run_id}"
    dest.mkdir(parents=True, exist_ok=True)
    filename = "check_results.parquet" if folder == "dq_check_results" else "issue_log.parquet"
    path = dest / filename
    df.to_parquet(path, index=False)
    return path


def append_run_history(
    run_id: str, run_ts: datetime, dataset_name: str, metadata: StageMetadata
) -> Path:
    counts = {
        table.removeprefix("staging_"): metadata.table_counts.get(table, 0)
        for table in metadata.table_counts
    }
    entry = pd.DataFrame(
        [
            {
                "run_id": run_id,
                "run_ts": run_ts.isoformat(),
                "dataset_name": dataset_name,
                "total_rows_by_table": json.dumps(counts, ensure_ascii=False),
            }
        ]
    )
    return _append_history_table(entry, "dq_run_history", "run_history.parquet")


def append_issue_history(issue_log: pd.DataFrame) -> Path:
    return _append_history_table(issue_log, "dq_issue_history", "issue_history.parquet")


def persist_recurrence_summary(df: pd.DataFrame, run_id: str) -> Path:
    dest = DATA_MARTS_BASE / "dq_issue_recurrence" / f"run_id={run_id}"
    dest.mkdir(parents=True, exist_ok=True)
    path = dest / "top_recurring_issues.parquet"
    df.to_parquet(path, index=False)
    return path


def _append_history_table(df: pd.DataFrame, folder: str, filename: str) -> Path:
    dest_dir = DATA_MARTS_BASE / folder
    dest_dir.mkdir(parents=True, exist_ok=True)
    path = dest_dir / filename
    if path.exists():
        existing = pd.read_parquet(path)
        df = pd.concat([existing, df], ignore_index=True)
    df.to_parquet(path, index=False)
    return path
</file>

<file path="dq/validate/output_recurrence.py">
"""Recurrence metrics helpers for validation output."""

from __future__ import annotations

import pandas as pd

from dq.validate.paths import DATA_MARTS_BASE


def compute_recurrence_metrics(limit: int = 10) -> pd.DataFrame:
    history_path = DATA_MARTS_BASE / "dq_issue_history" / "issue_history.parquet"
    if not history_path.exists():
        return _empty_recurrence_df()
    history = pd.read_parquet(history_path)
    if history.empty:
        return _empty_recurrence_df()
    working = history.copy()
    working["run_ts"] = pd.to_datetime(working["run_ts"])
    grouped = (
        working.groupby(["check_name", "table_name", "issue_type"], as_index=False)
        .agg(
            occurrences=("run_id", "count"),
            median_affected_pct=("affected_pct", "median"),
            last_seen=("run_ts", "max"),
            probable_root_cause=("probable_root_cause", "last"),
            recommended_fix=("recommended_fix", "last"),
        )
        .sort_values(["occurrences", "last_seen"], ascending=[False, False])
        .head(limit)
    )
    grouped["last_seen"] = grouped["last_seen"].apply(
        lambda ts: ts.isoformat() if pd.notna(ts) else ""
    )
    columns = [
        "check_name",
        "table_name",
        "issue_type",
        "occurrences",
        "median_affected_pct",
        "last_seen",
        "probable_root_cause",
        "recommended_fix",
    ]
    return grouped[columns]


def _empty_recurrence_df() -> pd.DataFrame:
    columns = [
        "check_name",
        "table_name",
        "issue_type",
        "occurrences",
        "median_affected_pct",
        "last_seen",
        "probable_root_cause",
        "recommended_fix",
    ]
    return pd.DataFrame(columns=columns)
</file>

<file path="dq/validate/paths.py">
"""Path helpers for validation outputs."""

from __future__ import annotations

from pathlib import Path


REPO_ROOT = Path(__file__).resolve().parents[2]
DATA_MARTS_BASE = REPO_ROOT / "data" / "marts"
GE_ARTIFACTS_BASE = REPO_ROOT / "dq" / "ge"
</file>

<file path="dq/validate/penalty_utils.py">
"""Penalty and status helpers for rule evaluations."""

from __future__ import annotations

from dq.validate.config import CheckRule


class PenaltyMixin:
    @staticmethod
    def _compute_penalty(rule: CheckRule, failure_rate: float) -> float:
        return failure_rate * (rule.severity / 5) * rule.weight

    @staticmethod
    def _determine_status(rule: CheckRule, failure_rate: float) -> str:
        if failure_rate >= rule.threshold.fail:
            return "fail"
        if failure_rate >= rule.threshold.warning:
            return "warn"
        return "pass"
</file>

<file path="dq/validate/query_utils.py">
"""DuckDB execution helpers for rule evaluation."""

from __future__ import annotations

from typing import Any

import duckdb

from dq.validate.config import CheckRule
from dq.validate.constants import ISSUE_TYPE_MAP, SAMPLE_LIMIT
from dq.validate.models import CheckResult
from scripts.profile_utils import quote_ident, stringify_value


class QueryExecutorMixin:
    def _execute_condition(
        self, con: duckdb.DuckDBPyConnection, rule: CheckRule, stage_table: str, condition: str
    ) -> CheckResult:
        query = f"SELECT * FROM {quote_ident(stage_table)} WHERE {condition}"
        return self._execute_query_result(con, rule, stage_table, query)

    def _execute_query_result(
        self, con: duckdb.DuckDBPyConnection, rule: CheckRule, stage_table: str, query: str
    ) -> CheckResult:
        failure_count = self._count_matches(con, query)
        samples = self._fetch_samples(con, query, SAMPLE_LIMIT)
        total_rows = getattr(self, "_table_counts", {}).get(stage_table, 0)
        failure_rate = failure_count / total_rows if total_rows else 0.0
        return CheckResult(
            rule=rule,
            table=rule.table,
            stage_table=stage_table,
            failure_count=failure_count,
            total_rows=total_rows,
            failure_rate=failure_rate,
            penalty=self._compute_penalty(rule, failure_rate),
            status=self._determine_status(rule, failure_rate),
            issue_type=ISSUE_TYPE_MAP.get(rule.rule_type, "invalid"),
            samples=samples,
        )

    @staticmethod
    def _count_matches(con: duckdb.DuckDBPyConnection, query: str) -> int:
        return int(con.execute(f"SELECT COUNT(*) FROM ({query}) AS dq_failures").fetchone()[0])

    @staticmethod
    def _fetch_samples(
        con: duckdb.DuckDBPyConnection, query: str, limit: int
    ) -> list[dict[str, Any]]:
        cursor = con.execute(f"SELECT * FROM ({query}) AS dq_samples LIMIT {limit}")
        columns = [desc[0] for desc in cursor.description] if cursor.description else []
        return [
            {col: stringify_value(value) for col, value in zip(columns, row)}
            for row in cursor.fetchall()
        ]
</file>

<file path="dq/validate/rule_executor.py">
"""DuckDB rule evaluator for validation checks."""

from __future__ import annotations

import re
from typing import Any

import duckdb

from dq.validate.config import CheckRule
from dq.validate.constants import ISSUE_TYPE_MAP, SAMPLE_LIMIT
from dq.validate.metadata import StageMetadata
from dq.validate.models import CheckResult
from dq.validate.penalty_utils import PenaltyMixin
from dq.validate.query_utils import QueryExecutorMixin
from dq.validate.stage_utils import StageResolverMixin
from scripts.profile_utils import quote_ident, quote_literal


class RuleEvaluator(
    StageResolverMixin, QueryExecutorMixin, PenaltyMixin
):
    def __init__(self, metadata: StageMetadata) -> None:
        self._table_map = metadata.table_map
        self._table_columns = metadata.table_columns
        self._table_counts = metadata.table_counts

    def evaluate(
        self, con: duckdb.DuckDBPyConnection, rule: CheckRule
    ) -> CheckResult:
        handler = {
            "NULL_PERCENTAGE": self._handle_null_percentage,
            "PATTERN": self._handle_pattern,
            "DATE_RANGE": self._handle_date_range,
            "NON_NEGATIVE_COUNTS": self._handle_non_negative_counts,
            "TIMESTAMP_ORDER": self._handle_timestamp_order,
            "ENUM": self._handle_enum,
            "UNIQUE_MAPPING": self._handle_unique_mapping,
            "DUPLICATE_PERCENTAGE": self._handle_duplicate_percentage,
            "FK": self._handle_foreign_key,
        }.get(rule.rule_type.upper())
        if not handler:
            raise RuntimeError(f"No handler for rule {rule.rule_type}")
        return handler(con, rule)

    def _handle_null_percentage(
        self, con: duckdb.DuckDBPyConnection, rule: CheckRule
    ) -> CheckResult:
        stage_table = self._resolve_stage_table(rule.table)
        column = rule.columns and rule.columns[0]
        condition = f"{quote_ident(column)} IS NULL"
        return self._execute_condition(con, rule, stage_table, condition)

    def _handle_pattern(
        self, con: duckdb.DuckDBPyConnection, rule: CheckRule
    ) -> CheckResult:
        stage_table = self._resolve_stage_table(rule.table)
        column = rule.columns and rule.columns[0]
        pattern = rule.rule_args[0]
        condition = (
            f"{quote_ident(column)} IS NOT NULL AND NOT REGEXP_MATCHES(TRIM({quote_ident(column)}), {quote_literal(pattern)})"
        )
        return self._execute_condition(con, rule, stage_table, condition)

    def _handle_date_range(
        self, con: duckdb.DuckDBPyConnection, rule: CheckRule
    ) -> CheckResult:
        stage_table = self._resolve_stage_table(rule.table)
        column = rule.columns and rule.columns[0]
        start_literal = self._format_timestamp_literal(rule.rule_args[0])
        end_literal = self._format_timestamp_literal(rule.rule_args[1])
        clauses: list[str] = []
        if start_literal:
            clauses.append(f"{quote_ident(column)} < {start_literal}")
        if end_literal:
            clauses.append(f"{quote_ident(column)} > {end_literal}")
        condition = (
            f"{quote_ident(column)} IS NOT NULL AND ({' OR '.join(clauses)})"
        ) if clauses else "0=1"
        return self._execute_condition(con, rule, stage_table, condition)

    def _handle_non_negative_counts(
        self, con: duckdb.DuckDBPyConnection, rule: CheckRule
    ) -> CheckResult:
        matches: list[tuple[str, str]] = []
        pattern = rule.column_regex or ""
        regex = re.compile(pattern, re.IGNORECASE)
        for stage_table, columns in self._table_columns.items():
            for column in columns:
                if regex.search(column):
                    matches.append((stage_table, column))
        failure_count = 0
        total_rows = 0
        samples: list[dict[str, Any]] = []
        for stage_table, column in matches:
            total_rows += self._table_counts.get(stage_table, 0)
            query = (
                f"SELECT *, '{column}' AS failed_column FROM {quote_ident(stage_table)} "
                f"WHERE {quote_ident(column)} < 0"
            )
            batch_count = self._count_matches(con, query)
            failure_count += batch_count
            if len(samples) < SAMPLE_LIMIT:
                samples.extend(
                    self._fetch_samples(con, query, SAMPLE_LIMIT - len(samples))
                )
        failure_rate = failure_count / total_rows if total_rows else 0.0
        return CheckResult(
            rule=rule,
            table=rule.table,
            stage_table="*",
            failure_count=failure_count,
            total_rows=total_rows,
            failure_rate=failure_rate,
            penalty=self._compute_penalty(rule, failure_rate),
            status=self._determine_status(rule, failure_rate),
            issue_type=ISSUE_TYPE_MAP.get(rule.rule_type, "invalid"),
            samples=samples,
        )

    def _handle_timestamp_order(
        self, con: duckdb.DuckDBPyConnection, rule: CheckRule
    ) -> CheckResult:
        stage_table = self._resolve_stage_table(rule.table)
        columns = rule.columns or []
        clauses = [
            f"{quote_ident(nxt)} IS NOT NULL AND ({quote_ident(prev)} IS NULL OR {quote_ident(prev)} > {quote_ident(nxt)})"
            for prev, nxt in zip(columns, columns[1:])
        ]
        condition = " OR ".join(clauses) if clauses else "0=1"
        return self._execute_condition(con, rule, stage_table, condition)

    def _handle_enum(
        self, con: duckdb.DuckDBPyConnection, rule: CheckRule
    ) -> CheckResult:
        stage_table = self._resolve_stage_table(rule.table)
        column = rule.columns and rule.columns[0]
        allowed = ", ".join(quote_literal(val.lower()) for val in rule.rule_args)
        condition = (
            f"{quote_ident(column)} IS NOT NULL AND LOWER(TRIM({quote_ident(column)})) NOT IN ({allowed})"
        )
        return self._execute_condition(con, rule, stage_table, condition)

    def _handle_unique_mapping(
        self, con: duckdb.DuckDBPyConnection, rule: CheckRule
    ) -> CheckResult:
        stage_table = self._resolve_stage_table(rule.table)
        key_col, value_col = rule.rule_args
        failure_query = f"""
        WITH inconsistent AS (
            SELECT {quote_ident(key_col)}
            FROM {quote_ident(stage_table)}
            WHERE {quote_ident(key_col)} IS NOT NULL
            GROUP BY {quote_ident(key_col)}
            HAVING COUNT(DISTINCT {quote_ident(value_col)}) > 1
        )
        SELECT t.*
        FROM {quote_ident(stage_table)} t
        JOIN inconsistent i ON i.{quote_ident(key_col)} = t.{quote_ident(key_col)}
        """
        return self._execute_query_result(con, rule, stage_table, failure_query)

    def _handle_duplicate_percentage(
        self, con: duckdb.DuckDBPyConnection, rule: CheckRule
    ) -> CheckResult:
        stage_table = self._resolve_stage_table(rule.table)
        columns = rule.columns or []
        partition_cols = ", ".join(quote_ident(col) for col in columns)
        failure_query = f"""
        SELECT *
        FROM (
            SELECT *, ROW_NUMBER() OVER (PARTITION BY {partition_cols} ORDER BY {quote_ident(columns[0])}) AS dq_rank
            FROM {quote_ident(stage_table)}
        ) dq
        WHERE dq.dq_rank > 1
        """
        return self._execute_query_result(con, rule, stage_table, failure_query)

    def _handle_foreign_key(
        self, con: duckdb.DuckDBPyConnection, rule: CheckRule
    ) -> CheckResult:
        stage_table = self._resolve_stage_table(rule.table)
        column = rule.columns and rule.columns[0]
        ref_table_name, ref_column = rule.rule_args[0].split(".")
        ref_stage = self._resolve_stage_table(ref_table_name)
        failure_query = f"""
        SELECT src.*
        FROM {quote_ident(stage_table)} src
        LEFT JOIN {quote_ident(ref_stage)} ref
          ON src.{quote_ident(column)} = ref.{quote_ident(ref_column)}
        WHERE src.{quote_ident(column)} IS NOT NULL
          AND ref.{quote_ident(ref_column)} IS NULL
        """
        return self._execute_query_result(con, rule, stage_table, failure_query)
</file>

<file path="dq/validate/scoring.py">
"""Score calculations for validation runs."""

from __future__ import annotations

from collections.abc import Iterable
from typing import Dict, Tuple

from dq.validate.models import CheckResult


def calculate_scores(
    results: Iterable[CheckResult], baseline: float, minimum: float
) -> Tuple[float, Dict[str, float]]:
    total_weight = sum(result.rule.weight for result in results)
    penalties: Dict[str, float] = {}
    weights: Dict[str, float] = {}
    total_penalty = 0.0
    for result in results:
        total_penalty += result.penalty
        dim = result.rule.dimension
        penalties[dim] = penalties.get(dim, 0.0) + result.penalty
        weights[dim] = weights.get(dim, 0.0) + result.rule.weight
    normalized = total_penalty / total_weight if total_weight else 0.0
    score = max(minimum, baseline - 100 * normalized)
    subscores: Dict[str, float] = {}
    for dim, weight in weights.items():
        penalty = penalties.get(dim, 0.0)
        if not weight:
            continue
        subscores[dim] = max(minimum, baseline - 100 * (penalty / weight))
    return score, subscores
</file>

<file path="dq/validate/stage_utils.py">
"""Helpers for resolving stage tables and timestamps."""

from __future__ import annotations

from scripts.profile_utils import quote_literal


class StageResolverMixin:
    def _resolve_stage_table(self, logical_name: str) -> str:
        table_map = getattr(self, "_table_map", {})
        if logical_name in table_map:
            return table_map[logical_name]
        candidate = f"staging_{logical_name}"
        for table in table_map.values():
            if table == candidate:
                return table
        raise RuntimeError(f"Missing staging table for '{logical_name}'.")

    @staticmethod
    def _format_timestamp_literal(value: str) -> str | None:
        if not value:
            return None
        value = value.strip()
        if value.lower() == "now":
            return "CURRENT_TIMESTAMP"
        return f"TIMESTAMP {quote_literal(value)}"
</file>

<file path="dq/anomaly.py">
"""Lightweight anomaly detection on run-level metrics."""

from __future__ import annotations

import json
from collections import defaultdict
from dataclasses import dataclass
from itertools import chain
from pathlib import Path
from statistics import median
from typing import Iterable

import duckdb
import pandas as pd

from scripts.profile_utils import quote_ident
from dq.validate.paths import DATA_MARTS_BASE

METRICS_HISTORY_PATH = DATA_MARTS_BASE / "dq_metrics_history" / "metrics.parquet"
ANOMALIES_BASE = DATA_MARTS_BASE / "dq_anomalies"

METRICS_COLUMNS = [
    "run_id",
    "run_ts",
    "dataset_name",
    "event_volume",
    "completion_count",
    "completion_rate",
    "event_type_counts",
    "event_type_distribution",
]

EVENT_VOLUME_THRESHOLD = 3.0
COMPLETION_RATE_THRESHOLD = 3.0
DISTRIBUTION_SHIFT_THRESHOLD = 0.15


def run_anomaly_detection(
    run_id: str,
    dataset_name: str,
    run_ts: str,
    duckdb_path: Path,
) -> None:
    """Collect metrics for the current run, detect anomalies, and persist the results."""
    metrics = _collect_run_metrics(duckdb_path)
    record = {
        "run_id": run_id,
        "run_ts": run_ts,
        "dataset_name": dataset_name,
        "event_volume": metrics.event_volume,
        "completion_count": metrics.completion_count,
        "completion_rate": metrics.completion_rate,
        "event_type_counts": json.dumps(metrics.event_type_counts, ensure_ascii=False),
        "event_type_distribution": json.dumps(
            metrics.event_type_distribution, ensure_ascii=False
        ),
    }

    history = _load_metrics_history().query("dataset_name == @dataset_name")
    anomalies = _detect_anomalies(history, metrics, run_ts, dataset_name)

    _append_metrics_history(record)
    if anomalies:
        _persist_anomalies(run_id, anomalies)


def _collect_run_metrics(duckdb_path: Path) -> "RunMetrics":
    with duckdb.connect(str(duckdb_path)) as con:
        table = quote_ident("staging_events")
        event_volume = int(
            con.execute(f"SELECT COUNT(*) FROM {table}").fetchone()[0]
        )
        completion_count = int(
            con.execute(
                f"SELECT COUNT(*) FROM {table} WHERE event_type = 'complete'"
            ).fetchone()[0]
        )
        distribution_rows = con.execute(
            f"SELECT event_type, COUNT(*) FROM {table} GROUP BY event_type"
        ).fetchall()

    event_type_counts: dict[str, int] = {}
    event_type_distribution: dict[str, float] = {}
    if event_volume:
        for event_type, count in distribution_rows:
            key = event_type or ""
            event_type_counts[key] = int(count)
            event_type_distribution[key] = float(count) / event_volume
    completion_rate = (
        float(completion_count) / event_volume if event_volume else 0.0
    )
    return RunMetrics(
        event_volume=event_volume,
        completion_count=completion_count,
        completion_rate=completion_rate,
        event_type_counts=event_type_counts,
        event_type_distribution=event_type_distribution,
    )


def _load_metrics_history() -> pd.DataFrame:
    if not METRICS_HISTORY_PATH.exists():
        return pd.DataFrame(columns=METRICS_COLUMNS)
    return pd.read_parquet(METRICS_HISTORY_PATH)


def _append_metrics_history(record: dict[str, object]) -> None:
    METRICS_HISTORY_PATH.parent.mkdir(parents=True, exist_ok=True)
    frame = pd.DataFrame([record])
    if METRICS_HISTORY_PATH.exists():
        existing = pd.read_parquet(METRICS_HISTORY_PATH)
        frame = pd.concat([existing, frame], ignore_index=True)
    frame.to_parquet(METRICS_HISTORY_PATH, index=False)


def _detect_anomalies(
    history: pd.DataFrame,
    current: "RunMetrics",
    run_ts: str,
    dataset_name: str,
) -> list[dict[str, object]]:
    if history.empty:
        return []
    records: list[dict[str, object]] = []
    event_volume_series = history["event_volume"].dropna().astype(float)
    completion_rate_series = history["completion_rate"].dropna().astype(float)

    volume_anomaly = _check_value_anomaly(
        "event_volume",
        current.event_volume,
        event_volume_series,
        EVENT_VOLUME_THRESHOLD,
        direction="both",
        run_ts=run_ts,
        dataset_name=dataset_name,
    )
    if volume_anomaly:
        records.append(volume_anomaly)

    completion_anomaly = _check_value_anomaly(
        "completion_rate",
        current.completion_rate,
        completion_rate_series,
        COMPLETION_RATE_THRESHOLD,
        direction="down",
        run_ts=run_ts,
        dataset_name=dataset_name,
    )
    if completion_anomaly:
        records.append(completion_anomaly)

    distribution_anomaly = _check_distribution_shift(
        history, current, run_ts, dataset_name
    )
    if distribution_anomaly:
        records.append(distribution_anomaly)

    return records


def _check_value_anomaly(
    metric: str,
    current_value: float,
    history: pd.Series,
    threshold: float,
    direction: str,
    run_ts: str,
    dataset_name: str,
) -> dict[str, object] | None:
    if history.empty:
        return None
    median_value = history.median()
    mad = (history - median_value).abs().median()
    if mad == 0:
        if current_value == median_value:
            return None
        notes = (
            f"{metric} deviated from median {median_value:.2f} without variation."
        )
        return _build_record(
            metric,
            current_value,
            median_value,
            mad,
            None,
            threshold,
            direction,
            notes,
            details={
                "median": median_value,
                "current": current_value,
                "mad": mad,
            },
            run_ts=run_ts,
            dataset_name=dataset_name,
        )
    z_score = (current_value - median_value) / mad
    if direction == "down" and z_score > 0:
        return None
    if abs(z_score) < threshold:
        return None
    notes = (
        f"{metric} moved {direction} to {current_value:.4f} (median {median_value:.4f}, mad {mad:.4f})."
    )
    return _build_record(
        metric,
        current_value,
        median_value,
        mad,
        z_score,
        threshold,
        direction,
        notes,
        details={
            "median": median_value,
            "mad": mad,
            "z_score": z_score,
        },
        run_ts=run_ts,
        dataset_name=dataset_name,
    )


def _check_distribution_shift(
    history: pd.DataFrame,
    current: "RunMetrics",
    run_ts: str,
    dataset_name: str,
) -> dict[str, object] | None:
    dist_column = history["event_type_distribution"].dropna()
    if dist_column.empty or not current.event_type_distribution:
        return None
    baseline = _aggregate_median_distribution(dist_column)
    if not baseline:
        return None
    current_dist = current.event_type_distribution
    keys = set(baseline) | set(current_dist)
    deltas = {
        key: abs(current_dist.get(key, 0.0) - baseline.get(key, 0.0))
        for key in keys
    }
    key, delta = max(deltas.items(), key=lambda item: item[1])
    if delta < DISTRIBUTION_SHIFT_THRESHOLD:
        return None
    notes = (
        f"Event type '{key or 'missing'}' shifted by {delta:.2%} relative to baseline."
    )
    details = {
        "category": key,
        "baseline": baseline.get(key, 0.0),
        "current": current_dist.get(key, 0.0),
        "delta": delta,
    }
    return _build_record(
        "event_type_distribution",
        delta,
        baseline.get(key, 0.0),
        DISTRIBUTION_SHIFT_THRESHOLD,
        None,
        DISTRIBUTION_SHIFT_THRESHOLD,
        "shift",
        notes,
        details,
        run_ts=run_ts,
        dataset_name=dataset_name,
    )


def _aggregate_median_distribution(series: Iterable[str]) -> dict[str, float]:
    buckets: dict[str, list[float]] = defaultdict(list)
    for raw in series:
        try:
            values = json.loads(raw)
        except json.JSONDecodeError:
            continue
        for key, value in values.items():
            buckets[key].append(float(value))
    return {key: median(vals) for key, vals in buckets.items() if vals}


def _build_record(
    metric: str,
    metric_value: float,
    baseline_value: float,
    baseline_spread: float,
    z_score: float | None,
    threshold: float,
    direction: str,
    notes: str,
    details: dict[str, object],
    run_ts: str,
    dataset_name: str,
) -> dict[str, object]:
    return {
        "metric": metric,
        "metric_value": float(metric_value),
        "baseline_value": float(baseline_value),
        "baseline_spread": float(baseline_spread),
        "z_score": float(z_score) if z_score is not None else None,
        "threshold": float(threshold),
        "direction": direction,
        "notes": notes,
        "details": json.dumps(details, ensure_ascii=False),
        "run_ts": run_ts,
        "dataset_name": dataset_name,
    }


def _persist_anomalies(run_id: str, records: list[dict[str, object]]) -> None:
    payload = []
    for record in records:
        payload.append(
            {
                "run_id": run_id,
                "metric": record["metric"],
                "metric_value": record["metric_value"],
                "baseline_value": record["baseline_value"],
                "baseline_spread": record["baseline_spread"],
                "z_score": record["z_score"],
                "threshold": record["threshold"],
                "direction": record["direction"],
                "notes": record["notes"],
                "details": record["details"],
                "dataset_name": record.get("dataset_name"),
                "run_ts": record.get("run_ts"),
            }
        )
    payload_frame = pd.DataFrame(payload)
    ANOMALIES_BASE.mkdir(parents=True, exist_ok=True)
    run_dir = ANOMALIES_BASE / f"run_id={run_id}"
    run_dir.mkdir(parents=True, exist_ok=True)
    payload_frame.to_parquet(run_dir / "anomalies.parquet", index=False)

    summary_path = ANOMALIES_BASE / "anomalies.parquet"
    if summary_path.exists():
        existing = pd.read_parquet(summary_path)
        payload_frame = pd.concat([existing, payload_frame], ignore_index=True)
    payload_frame.to_parquet(summary_path, index=False)


def _export_dataset(record: dict[str, object]) -> dict[str, object]:
    """Ensure dataset_name is added to anomaly records."""
    return record


@dataclass(frozen=True)
class RunMetrics:
    event_volume: int
    completion_count: int
    completion_rate: float
    event_type_counts: dict[str, int]
    event_type_distribution: dict[str, float]
</file>

<file path="dq/schema_drift.py">
"""Detect schema drift between deployed schema and inferred staging tables."""

from __future__ import annotations

import json
from pathlib import Path
from typing import Dict, Iterable

import duckdb
import pandas as pd
import yaml

from dq.validate.paths import DATA_MARTS_BASE

SCHEMA_CONFIG_PATH = Path(__file__).resolve().parents[1] / "dq" / "config" / "schema.yml"
SCHEMA_DRIFT_BASE = DATA_MARTS_BASE / "dq_schema_drift"

TYPE_CANONICAL: dict[str, str] = {
    "uuid": "varchar",
    "email": "varchar",
    "enum": "varchar",
    "state_code": "varchar",
    "string": "varchar",
    "timestamp": "timestamp",
    "int": "integer",
    "integer": "integer",
    "float": "float",
    "decimal": "float",
}


def run_schema_drift_detection(
    run_id: str,
    dataset_name: str,
    run_ts: str,
    stage_path: Path,
    duckdb_path: Path,
) -> None:
    """Persist schema drift observations for the current run."""
    expected = _load_expected_schema()
    actual = _collect_inferred_schema(duckdb_path)

    records = _compare(expected, actual)
    if not records:
        return

    _persist_schema_drift(run_id, dataset_name, run_ts, records)


def _load_expected_schema() -> dict[str, dict[str, str]]:
    if not SCHEMA_CONFIG_PATH.exists():
        return {}
    raw = yaml.safe_load(SCHEMA_CONFIG_PATH.read_text()) or {}
    tables = raw.get("tables") or {}
    schema: dict[str, dict[str, str]] = {}
    for table_name, payload in tables.items():
        columns = payload.get("columns") or {}
        schema[table_name] = {
            column_name: str(column_data.get("type", ""))
            if isinstance(column_data, dict)
            else str(column_data)
            for column_name, column_data in columns.items()
        }
    return schema


def _collect_inferred_schema(duckdb_path: Path) -> dict[str, dict[str, str]]:
    with duckdb.connect(str(duckdb_path)) as con:
        rows = con.execute(
            """
            SELECT table_name, column_name, data_type
            FROM information_schema.columns
            WHERE table_schema = 'main'
              AND table_name LIKE 'staging_%'
            ORDER BY table_name, ordinal_position
            """
        ).fetchall()

    schema: dict[str, dict[str, str]] = {}
    for table_name, column_name, data_type in rows:
        logical = table_name.removeprefix("staging_")
        schema.setdefault(logical, {})[column_name] = data_type.lower()
    return schema


def _compare(
    expected: dict[str, dict[str, str]],
    actual: dict[str, dict[str, str]],
) -> list[dict[str, object]]:
    records: list[dict[str, object]] = []
    expected_tables = set(expected)
    actual_tables = set(actual)

    for table in sorted(expected_tables - actual_tables):
        records.append(
            _build_record(
                table,
                missing_columns=sorted(expected.get(table, {}).keys()),
                new_columns=[],
                type_changes=[],
                notes="Expected staging table was not created.",
            )
        )

    for table in sorted(actual_tables - expected_tables):
        cols = sorted(actual.get(table, {}).keys())
        records.append(
            _build_record(
                table,
                missing_columns=[],
                new_columns=cols,
                type_changes=[],
                notes="Unexpected staging table appeared.",
            )
        )

    for table in sorted(expected_tables & actual_tables):
        expected_cols = expected.get(table, {})
        actual_cols = actual.get(table, {})
        missing = sorted(set(expected_cols) - set(actual_cols))
        new = sorted(set(actual_cols) - set(expected_cols))
        type_changes = []
        for column in sorted(set(expected_cols) & set(actual_cols)):
            expected_type = _normalize_config_type(expected_cols[column])
            actual_type = _normalize_actual_type(actual_cols[column])
            if expected_type and actual_type and expected_type != actual_type:
                type_changes.append(
                    {
                        "column": column,
                        "expected": expected_type,
                        "actual": actual_type,
                    }
                )
        if missing or new or type_changes:
            notes_parts: list[str] = []
            if missing:
                notes_parts.append(f"Missing columns: {', '.join(missing)}")
            if new:
                notes_parts.append(f"New columns: {', '.join(new)}")
            if type_changes:
                diffs = ", ".join(
                    f"{d['column']} ({d['expected']}→{d['actual']})" for d in type_changes
                )
                notes_parts.append(f"Type changes: {diffs}")
            records.append(
                _build_record(
                    table,
                    missing_columns=missing,
                    new_columns=new,
                    type_changes=type_changes,
                    notes="; ".join(notes_parts),
                )
            )
    return records


def _normalize_config_type(value: str | None) -> str | None:
    if not value:
        return None
    key = value.strip().lower()
    return TYPE_CANONICAL.get(key, key)


def _normalize_actual_type(value: str | None) -> str | None:
    if not value:
        return None
    return value.strip().lower()


def _build_record(
    table: str,
    missing_columns: Iterable[str],
    new_columns: Iterable[str],
    type_changes: Iterable[dict[str, str]],
    notes: str,
) -> dict[str, object]:
    return {
        "table_name": table,
        "missing_columns": json.dumps(list(missing_columns), ensure_ascii=False),
        "new_columns": json.dumps(list(new_columns), ensure_ascii=False),
        "type_changes": json.dumps(list(type_changes), ensure_ascii=False),
        "notes": notes,
    }


def _persist_schema_drift(
    run_id: str,
    dataset_name: str,
    run_ts: str,
    records: list[dict[str, object]],
) -> None:
    payload = []
    for record in records:
        payload.append(
            {
                "run_id": run_id,
                "run_ts": run_ts,
                "dataset_name": dataset_name,
                **record,
            }
        )
    frame = pd.DataFrame(payload)
    run_dir = SCHEMA_DRIFT_BASE / f"run_id={run_id}"
    run_dir.mkdir(parents=True, exist_ok=True)
    frame.to_parquet(run_dir / "drift.parquet", index=False)

    summary_path = SCHEMA_DRIFT_BASE / "drift.parquet"
    if summary_path.exists():
        existing = pd.read_parquet(summary_path)
        frame = pd.concat([existing, frame], ignore_index=True)
    frame.to_parquet(summary_path, index=False)
</file>

<file path="reports/templates/scorecard.html.jinja">
<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DQ Scorecard</title>
  <style>
    :root {
      font-family: "Inter", "Segoe UI", system-ui, -apple-system, sans-serif;
      color: #0f172a;
    }
    body {
      margin: 0;
      background: #f4f6fb;
    }
    .page {
      max-width: 1000px;
      margin: 0 auto;
      padding: 2rem;
    }
    header.hero {
      background: #111827;
      color: #fff;
      border-radius: 16px;
      padding: 2rem;
      margin-bottom: 2rem;
    }
    .score-box {
      display: flex;
      align-items: baseline;
      gap: 0.5rem;
      margin-top: 0.5rem;
    }
    .score-value {
      font-size: 3rem;
      font-weight: 700;
    }
    .score-label {
      font-size: 1rem;
      color: #d1d5db;
    }
    .hero meta {
      color: #cbd5f5;
    }
    section {
      background: #fff;
      border-radius: 16px;
      padding: 1.5rem;
      margin-bottom: 1.5rem;
      box-shadow: 0 10px 25px rgba(15, 23, 42, 0.08);
    }
    h1 {
      margin: 0;
      font-size: 2rem;
    }
    h2 {
      margin-top: 0;
      font-size: 1.4rem;
      border-bottom: 1px solid #e0e7ff;
      padding-bottom: 0.25rem;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 0.75rem;
    }
    table th,
    table td {
      border: 1px solid #e5e7eb;
      padding: 0.5rem;
      text-align: left;
      vertical-align: middle;
    }
    table th {
      background: #f8fafc;
      font-size: 0.85rem;
      text-transform: uppercase;
      letter-spacing: 0.05em;
    }
    .grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
      gap: 1rem;
      margin-top: 1rem;
    }
    .grid article {
      background: #111827;
      color: #fff;
      border-radius: 12px;
      padding: 1rem;
      display: flex;
      flex-direction: column;
      gap: 0.4rem;
    }
    .grid article h3 {
      margin: 0;
      font-size: 0.9rem;
      text-transform: uppercase;
      letter-spacing: 0.1em;
      color: #c7d2fe;
    }
    .grid article strong {
      font-size: 1.8rem;
    }
    .badge {
      display: inline-flex;
      align-items: center;
      padding: 0.35rem 0.75rem;
      border-radius: 999px;
      background: rgba(255, 255, 255, 0.15);
      font-size: 0.85rem;
      letter-spacing: 0.08em;
      text-transform: uppercase;
    }
    .meta {
      font-size: 0.95rem;
      color: #e2e8f0;
      margin-top: 0.5rem;
    }
    .section-note {
      margin: 0;
      font-size: 0.9rem;
      color: #475569;
    }
    .chart-placeholder {
      text-align: center;
      color: #94a3b8;
    }
    .chart {
      margin-top: 1rem;
      border: 1px solid #e5e7eb;
      border-radius: 12px;
      padding: 1rem;
      background: #f8fafc;
    }
    .chart svg {
      max-width: 100%;
      height: auto;
    }
    .chart-legend {
      display: flex;
      flex-wrap: wrap;
      gap: 0.6rem;
      margin-top: 0.75rem;
    }
    .chart-legend span {
      display: inline-flex;
      align-items: center;
      gap: 0.35rem;
      border: 1px solid #e5e7eb;
      border-radius: 999px;
      padding: 0.2rem 0.6rem;
      font-size: 0.85rem;
      background: #fff;
    }
    .chart-legend .swatch {
      width: 14px;
      height: 14px;
      border-radius: 4px;
      display: inline-block;
    }
    .badge + .meta {
      margin-top: 0.75rem;
    }
    .issue-table a {
      color: #1d4ed8;
      text-decoration: none;
    }
    .issue-totals {
      list-style: none;
      padding: 0;
      margin: 0.5rem 0 0;
      display: flex;
      flex-wrap: wrap;
      gap: 0.75rem;
      font-weight: 600;
    }
    .chart-empty {
      color: #475569;
      text-align: center;
      padding: 1rem;
    }
  </style>
</head>
<body>
  <div class="page">
    <header class="hero">
      <span class="badge">Latest run</span>
      <h1>DQ Scorecard</h1>
      <p class="meta">
        Dataset {{ score_data.dataset_name }} · Run {{ score_data.run_id }} · {{ score_data.run_ts }}
        · Generated at {{ score_data.generated_at }}
      </p>
      <div class="score-box">
        <span class="score-value">{{ score_data.score | round(1) }}</span>
        <span class="score-label">Overall data quality score</span>
      </div>
      <p class="meta">
        {{ score_data.total_checks }} checks · {{ score_data.failed_checks }} issues ·
        Baseline {{ score_data.baseline }} · Floor {{ score_data.minimum }}
      </p>
    </header>

    <section>
      <h2>Dimension subscores</h2>
      <div class="grid">
        {% for dimension, value in score_data.subscores.items() %}
        <article>
          <h3>{{ dimension.replace('_', ' ') | title }}</h3>
          <strong>{{ value | round(1) }}</strong>
          <span class="section-note">Higher is better</span>
        </article>
        {% endfor %}
      </div>
    </section>

    <section>
      <h2>Checks summary</h2>
      <p class="section-note">
        Displaying {{ check_rows | length }} of {{ score_data.total_checks }} checks.
      </p>
      <table>
        <thead>
          <tr>
            <th>Check</th>
            <th>Table</th>
            <th>Dimension</th>
            <th>Status</th>
            <th>Failure %</th>
            <th>Severity</th>
            <th>Weight</th>
          </tr>
        </thead>
        <tbody>
          {% for check in check_rows %}
          <tr>
            <td>{{ check.check_id }}</td>
            <td>{{ check.table_name }}</td>
            <td>{{ check.dimension }}</td>
            <td>{{ check.status }}</td>
            <td>{{ (check.failure_rate * 100) | round(2) }}%</td>
            <td>{{ check.severity }}</td>
            <td>{{ check.weight | round(1) }}</td>
          </tr>
          {% endfor %}
        </tbody>
      </table>
    </section>

    <section>
      <h2>Issue log</h2>
      <p class="section-note">
        Issues detected: {{ score_data.failed_checks }} ·
        <a href="issues.csv" download>Download CSV</a>
      </p>
      {% if issue_rows %}
      <table class="issue-table">
        <thead>
          <tr>
            <th>Check</th>
            <th>Issue type</th>
            <th>Severity</th>
            <th>Affected rows %</th>
            <th>Root cause</th>
            <th>Recommended fix</th>
          </tr>
        </thead>
        <tbody>
          {% for issue in issue_rows %}
          <tr>
            <td>{{ issue.check_name }}</td>
            <td>{{ issue.issue_type }}</td>
            <td>{{ issue.severity }}</td>
            <td>{{ (issue.affected_pct * 100) | round(2) }}%</td>
            <td>{{ issue.probable_root_cause }}</td>
            <td>{{ issue.recommended_fix }}</td>
          </tr>
          {% endfor %}
        </tbody>
      </table>
      {% else %}
      <p class="section-note">No issues recorded for this run.</p>
      {% endif %}
    </section>

    <section>
      <h2>Issue trends</h2>
      <div class="chart">
        {{ chart_svg | safe }}
      </div>
      {% if chart_legend %}
      <div class="chart-legend">
        {% for entry in chart_legend %}
        <span>
          <span class="swatch" style="background: {{ entry.color }}"></span>
          {{ entry.label }}
        </span>
        {% endfor %}
      </div>
      {% endif %}
      {% if issue_totals %}
      <ul class="issue-totals">
        {% for total in issue_totals[:6] %}
        <li>{{ total.issue_type }}: {{ total.count }}</li>
        {% endfor %}
      </ul>
      {% endif %}
    </section>

    <section>
      <h2>Recurring issues & SOP guidance</h2>
      {% if recurring_issues %}
      <table>
        <thead>
          <tr>
            <th>Check</th>
            <th>Table</th>
            <th>Issue Type</th>
            <th>Occurrences</th>
            <th>Median affected %</th>
            <th>Last seen</th>
            <th>Root cause</th>
            <th>Recommended fix</th>
          </tr>
        </thead>
        <tbody>
          {% for issue in recurring_issues %}
          <tr>
            <td>{{ issue.check_name }}</td>
            <td>{{ issue.table_name }}</td>
            <td>{{ issue.issue_type }}</td>
            <td>{{ issue.occurrences }}</td>
            <td>{{ issue.median_affected_pct | round(3) }}</td>
            <td>{{ issue.last_seen }}</td>
            <td>{{ issue.probable_root_cause }}</td>
            <td>{{ issue.recommended_fix }}</td>
          </tr>
          {% endfor %}
        </tbody>
      </table>
      {% else %}
      <p class="section-note">Not enough history yet to highlight recurring issues.</p>
      {% endif %}
    </section>
  </div>
</body>
</html>
</file>

<file path="scripts/publish_helpers/__init__.py">
"""Entry point for the publish helpers package."""

from __future__ import annotations

from .constants import (
    CHART_COLORS,
    ISSUE_HISTORY_PATH,
    LATEST_REPORT_DIR,
    REPORTS_BASE,
    REPO_ROOT,
    RUNS_REPORT_DIR,
    SVG_HEIGHT,
    SVG_MARGIN,
    SVG_WIDTH,
    TEMPLATE_DIR,
)
from .copy import copy_to_run_directory
from .history import mutate_context
from .io import load_score_payload
from .render import render_scorecard

__all__ = [
    "CHART_COLORS",
    "ISSUE_HISTORY_PATH",
    "LATEST_REPORT_DIR",
    "REPORTS_BASE",
    "REPO_ROOT",
    "RUNS_REPORT_DIR",
    "SVG_HEIGHT",
    "SVG_MARGIN",
    "SVG_WIDTH",
    "TEMPLATE_DIR",
    "copy_to_run_directory",
    "load_score_payload",
    "mutate_context",
    "render_scorecard",
]
</file>

<file path="scripts/publish_helpers/constants.py">
"""Shared constants used by the publish helpers."""

from __future__ import annotations

from pathlib import Path

from dq.validate.paths import DATA_MARTS_BASE

REPO_ROOT = Path(__file__).resolve().parents[1]
REPORTS_BASE = REPO_ROOT / "reports"
TEMPLATE_DIR = REPORTS_BASE / "templates"
LATEST_REPORT_DIR = REPORTS_BASE / "latest"
RUNS_REPORT_DIR = REPORTS_BASE / "runs"
ISSUE_HISTORY_PATH = DATA_MARTS_BASE / "dq_issue_history" / "issue_history.parquet"

SVG_WIDTH = 900
SVG_HEIGHT = 320
SVG_MARGIN = 50
CHART_COLORS = [
    "#1f77b4",
    "#ff7f0e",
    "#2ca02c",
    "#d62728",
    "#9467bd",
    "#8c564b",
    "#e377c2",
    "#7f7f7f",
]
</file>

<file path="scripts/publish_helpers/copy.py">
"""Copy helpers used by the publish helpers package."""

from __future__ import annotations

import shutil

from .constants import LATEST_REPORT_DIR, RUNS_REPORT_DIR


def copy_to_run_directory(run_id: str) -> None:
    run_dir = RUNS_REPORT_DIR / f"run_id={run_id}"
    run_dir.mkdir(parents=True, exist_ok=True)
    for artifact in ("index.html", "score.json", "issues.csv"):
        src = LATEST_REPORT_DIR / artifact
        if not src.exists():
            continue
        shutil.copy(src, run_dir / artifact)
</file>

<file path="scripts/publish_helpers/history.py">
"""Trend/history helpers for the publish helpers package."""

from __future__ import annotations

from typing import Any

import pandas as pd

from dq.validate.output import compute_recurrence_metrics

from .constants import (
    CHART_COLORS,
    ISSUE_HISTORY_PATH,
    SVG_HEIGHT,
    SVG_MARGIN,
    SVG_WIDTH,
)


def read_issue_history() -> pd.DataFrame:
    if not ISSUE_HISTORY_PATH.exists():
        return pd.DataFrame()
    try:
        history = pd.read_parquet(ISSUE_HISTORY_PATH)
        return history
    except Exception as exc:
        print(f"Warning: failed to read issue history ({exc}), trends will be skipped.")
        return pd.DataFrame()


def build_issue_totals(history: pd.DataFrame) -> list[dict[str, Any]]:
    if history.empty or "issue_type" not in history.columns:
        return []
    totals = (
        history["issue_type"]
        .fillna("Unknown")
        .value_counts()
        .rename_axis("issue_type")
        .reset_index(name="count")
    )
    return [
        {"issue_type": row["issue_type"], "count": int(row["count"]) }
        for _, row in totals.iterrows()
    ]


def build_trend_chart(history: pd.DataFrame) -> tuple[str, list[dict[str, str]]]:
    if history.empty:
        return (
            "<div class=\"chart-empty\">No issue history is available yet.</div>",
            [],
        )
    working = history.copy()
    working["issue_type"] = working["issue_type"].fillna("Unknown")
    working["run_ts_parsed"] = pd.to_datetime(working["run_ts"], errors="coerce")
    working["run_ts_parsed"] = working["run_ts_parsed"].fillna(pd.Timestamp("1970-01-01"))
    runs = (
        working[["run_id", "run_ts_parsed"]]
        .drop_duplicates(subset="run_id")
        .sort_values("run_ts_parsed")
    )
    if runs.empty:
        return (
            "<div class=\"chart-empty\">Issue history exists but run timestamps are missing.</div>",
            [],
        )
    counts = (
        working.groupby(["run_id", "issue_type"], dropna=False)
        .size()
        .reset_index(name="count")
    )
    totals = counts.groupby("issue_type")["count"].sum().sort_values(ascending=False)
    if totals.empty:
        return (
            "<div class=\"chart-empty\">Issue history exists but no issue types were detected.</div>",
            [],
        )
    top_types = totals.head(len(CHART_COLORS)).index.tolist()
    run_ids = runs["run_id"].tolist()
    count_map = {
        (row["run_id"], row["issue_type"]): int(row["count"])
        for _, row in counts.iterrows()
    }

    def y_position(value: float, max_value: float) -> float:
        span = SVG_HEIGHT - SVG_MARGIN * 2
        return SVG_HEIGHT - SVG_MARGIN - (value / max_value) * span if span else SVG_HEIGHT / 2

    max_value = max(
        (
            count_map.get((run_id, issue_type), 0)
            for run_id in run_ids
            for issue_type in top_types
        ),
        default=0,
    )
    max_value = max(max_value, 1)
    if len(run_ids) == 1:
        x_positions = [SVG_WIDTH / 2]
    else:
        step = (SVG_WIDTH - SVG_MARGIN * 2) / (len(run_ids) - 1)
        x_positions = [SVG_MARGIN + idx * step for idx in range(len(run_ids))]

    axis_lines = [
        f'<line x1="{SVG_MARGIN}" y1="{SVG_MARGIN}" x2="{SVG_MARGIN}" y2="{SVG_HEIGHT - SVG_MARGIN}" stroke="#333" stroke-width="1.2"/>',
        f'<line x1="{SVG_MARGIN}" y1="{SVG_HEIGHT - SVG_MARGIN}" x2="{SVG_WIDTH - SVG_MARGIN}" y2="{SVG_HEIGHT - SVG_MARGIN}" stroke="#333" stroke-width="1.2"/>',
    ]
    grid_lines: list[str] = []
    y_labels: list[str] = []
    for idx in range(5):
        value = (max_value * idx) / 4
        y = y_position(value, max_value)
        grid_lines.append(
            f'<line x1="{SVG_MARGIN}" y1="{y:.1f}" x2="{SVG_WIDTH - SVG_MARGIN}" y2="{y:.1f}" stroke="#ddd" stroke-dasharray="4,4"/>'
        )
        y_labels.append(
            f'<text x="{SVG_MARGIN - 10}" y="{y + 4:.1f}" text-anchor="end">{int(round(value))}</text>'
        )

    polylines: list[str] = []
    legend: list[dict[str, str]] = []
    for series_idx, issue_type in enumerate(top_types):
        values = [
            count_map.get((run_id, issue_type), 0) for run_id in run_ids
        ]
        color = CHART_COLORS[series_idx % len(CHART_COLORS)]
        legend.append({"label": issue_type, "color": color})
        points = " ".join(
            f"{x:.1f},{y_position(value, max_value):.1f}"
            for x, value in zip(x_positions, values)
        )
        polylines.append(
            f'<polyline fill="none" stroke="{color}" stroke-width="2" points="{points}" />'
        )

    x_labels: list[str] = []
    for idx, run_id in enumerate(run_ids):
        label_ts = runs.loc[runs["run_id"] == run_id, "run_ts_parsed"].iloc[0]
        label = f"{run_id} · {label_ts.strftime('%Y-%m-%d')}"
        x = x_positions[idx]
        x_labels.append(
            f'<text x="{x:.1f}" y="{SVG_HEIGHT - SVG_MARGIN + 20}" text-anchor="middle">{label}</text>'
        )

    chart_elements = "\n".join(grid_lines + axis_lines + y_labels + polylines + x_labels)
    chart_svg = (
        f'<svg width="{SVG_WIDTH}" height="{SVG_HEIGHT}" viewBox="0 0 {SVG_WIDTH} {SVG_HEIGHT}" '
        'role="img" aria-label="Issue trends by type over recent runs">'
        f"{chart_elements}"
        "</svg>"
    )
    return chart_svg, legend


def mutate_context(score_payload: dict[str, Any]) -> dict[str, Any]:
    chart_history = read_issue_history()
    chart_svg, legend = build_trend_chart(chart_history)
    issue_totals = build_issue_totals(chart_history)
    recurrence = compute_recurrence_metrics()
    return {
        "score_data": score_payload,
        "check_rows": score_payload.get("check_summary", [])[:30],
        "issue_rows": score_payload.get("issue_preview", []),
        "chart_svg": chart_svg,
        "chart_legend": legend,
        "recurring_issues": recurrence.to_dict(orient="records")
        if not recurrence.empty
        else [],
        "issue_totals": issue_totals,
    }
</file>

<file path="scripts/publish_helpers/io.py">
"""IO helpers for the publish helpers package."""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any


def load_score_payload(path: Path) -> dict[str, Any]:
    if not path.exists():
        raise SystemExit(f"Missing score payload at {path}")
    with path.open("r", encoding="utf-8") as handle:
        return json.load(handle)
</file>

<file path="scripts/publish_helpers/render.py">
"""Rendering helpers for the publish helpers package."""

from __future__ import annotations

from typing import Any

from jinja2 import Environment, FileSystemLoader, select_autoescape

from .constants import TEMPLATE_DIR


def render_scorecard(context: dict[str, Any]) -> str:
    env = Environment(
        loader=FileSystemLoader(str(TEMPLATE_DIR)),
        autoescape=select_autoescape(["html"]),
    )
    template = env.get_template("scorecard.html.jinja")
    return template.render(context)
</file>

<file path="scripts/__init__.py">

</file>

<file path="scripts/ingest_tables.py">
"""Store shared table definitions used by the ingest pipeline."""

TABLE_SPECS = [
    {
        "name": "districts",
        "source": "districts.csv",
        "select": """
        SELECT
          NULLIF(trim(district_id), '') AS district_id,
          NULLIF({district_case_expr}, '') AS district_name,
          NULLIF({state_case_expr}, '') AS state
        FROM read_csv_auto('{source_path}')""",
    },
    {
        "name": "users",
        "source": "users.csv",
        "select": """
        SELECT
          NULLIF(trim(user_id), '') AS user_id,
          NULLIF(lower(trim(email)), '') AS email,
          NULLIF(trim(org_id), '') AS org_id,
          NULLIF(lower(trim(role)), '') AS role,
          NULLIF({state_case_expr}, '') AS state,
          NULLIF(trim(district_id), '') AS district_id
        FROM read_csv_auto('{source_path}')""",
    },
    {
        "name": "resources",
        "source": "resources.csv",
        "select": """
        SELECT
          NULLIF(trim(resource_id), '') AS resource_id,
          NULLIF(lower(trim(type)), '') AS type,
          NULLIF(trim(subject), '') AS subject,
          NULLIF({grade_case_expr}, '') AS grade_band
        FROM read_csv_auto('{source_path}')""",
    },
    {
        "name": "events",
        "source": "events.csv",
        "select": """
        SELECT
          NULLIF(trim(event_id), '') AS event_id,
          NULLIF(trim(user_id), '') AS user_id,
          NULLIF(trim(resource_id), '') AS resource_id,
          NULLIF(lower(trim(event_type)), '') AS event_type,
          py_parse_ts(event_ts) AS event_ts
        FROM read_csv_auto('{source_path}')""",
    },
    {
        "name": "newsletter",
        "source": "newsletter.csv",
        "select": """
        SELECT
          NULLIF(lower(trim(email)), '') AS email,
          py_parse_ts(subscribed_at) AS subscribed_at,
          py_parse_ts(opened_at) AS opened_at,
          py_parse_ts(clicked_at) AS clicked_at
        FROM read_csv_auto('{source_path}')""",
    },
]
</file>

<file path="scripts/ingest.py">
#!/usr/bin/env python3
"""Entry point that orchestrates standardizing raw exports via the ingest library."""

import argparse
import textwrap

try:
    from scripts.ingest_lib import ingest_dataset
except ModuleNotFoundError:
    from ingest_lib import ingest_dataset


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Ingest raw CSV exports into cleaned staging tables and Parquet."
    )
    parser.add_argument("--dataset-name", default="phase1", help="Raw dataset folder.")
    parser.add_argument("--seed", type=int, default=42, help="Matching synthetic run seed.")
    parser.add_argument(
        "--force", action="store_true", help="Remove any existing staging output."
    )
    args = parser.parse_args()

    paths = ingest_dataset(args.dataset_name, args.seed, args.force)

    print(
        textwrap.dedent(
            f"""\
            Ingested {args.dataset_name} (seed={args.seed}) into {paths['stage_path']}.
            DuckDB file: {paths['db_path']}
            Parquet exports: {paths['parquet_path']}
            """
        ).strip()
    )


if __name__ == "__main__":
    main()
</file>

<file path="scripts/profile_collector.py">
"""Profile collector that orchestrates column profiling."""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Any

import duckdb

from scripts.profile_table import profile_table
from scripts.profile_utils import Record, collect_tables


@dataclass
class ProfileConfig:
    dataset_name: str
    seed: int
    stage_path: Path
    duckdb_path: Path
    run_metadata: dict[str, Any]
    run_id: str
    top_n: int


@dataclass
class ProfileResult:
    column_records: list[Record]
    top_value_records: list[Record]
    type_issue_records: list[Record]
    table_summary_records: list[Record]
    table_aggregates: dict[str, Record]


class ProfileCollector:
    def __init__(self, config: ProfileConfig) -> None:
        self.config = config

    def collect(self) -> ProfileResult:
        column_records: list[Record] = []
        top_value_records: list[Record] = []
        type_issue_records: list[Record] = []
        table_aggregates: dict[str, Record] = {}

        with duckdb.connect(str(self.config.duckdb_path)) as con:
            tables = collect_tables(con)
            if not tables:
                raise SystemExit("No staging tables were detected in the DuckDB file.")
            for table in tables:
                profile_table(
                    con,
                    table,
                    self.config.run_id,
                    self.config.dataset_name,
                    self.config.top_n,
                    column_records,
                    top_value_records,
                    type_issue_records,
                    table_aggregates,
                )

        table_summary_records = [
            {
                "run_id": self.config.run_id,
                "dataset_name": self.config.dataset_name,
                "table_name": entry["table_name"],
                "row_count": entry["row_count"],
                "column_count": entry["column_count"],
                "nulliest_column": entry["nulliest_column"],
                "max_null_rate": entry["max_null_rate"],
                "high_cardinality_column": entry["high_cardinality_column"],
                "max_cardinality": entry["max_cardinality"],
            }
            for entry in table_aggregates.values()
        ]
        return ProfileResult(
            column_records=column_records,
            top_value_records=top_value_records,
            type_issue_records=type_issue_records,
            table_summary_records=table_summary_records,
            table_aggregates=table_aggregates,
        )
</file>

<file path="scripts/profile_table.py">
"""Table-level profiling helpers."""

from __future__ import annotations

from typing import Any

import duckdb

from scripts.profile_utils import (
    Record,
    is_numeric_type,
    is_text_type,
    is_temporal_type,
    quote_ident,
    quote_literal,
    stringify_value,
)


def profile_table(
    con: duckdb.DuckDBPyConnection,
    table: str,
    run_id: str,
    dataset_name: str,
    top_n: int,
    column_records: list[Record],
    top_value_records: list[Record],
    type_issue_records: list[Record],
    table_aggregates: dict[str, Record],
) -> None:
    row_count = con.execute(f"SELECT COUNT(*) FROM {quote_ident(table)}").fetchone()[0]
    schema = con.execute(
        f"""
        SELECT column_name, data_type
        FROM information_schema.columns
        WHERE table_schema = 'main'
          AND table_name = {quote_literal(table)}
        ORDER BY ordinal_position
        """
    ).fetchall()
    logical_table = table.removeprefix("staging_")
    table_entry = table_aggregates.setdefault(
        logical_table,
        {
            "table_name": logical_table,
            "row_count": row_count,
            "column_count": len(schema),
            "max_null_rate": 0.0,
            "nulliest_column": None,
            "max_cardinality": 0,
            "high_cardinality_column": None,
        },
    )

    for column_name, column_type in schema:
        column_expr = quote_ident(column_name)
        null_count = con.execute(
            f"SELECT COUNT(*) FROM {quote_ident(table)} WHERE {column_expr} IS NULL"
        ).fetchone()[0]
        distinct_count = con.execute(
            f"SELECT COUNT(DISTINCT {column_expr}) FROM {quote_ident(table)}"
        ).fetchone()[0]

        min_value: Any = None
        max_value: Any = None
        if is_numeric_type(column_type) or is_temporal_type(column_type):
            min_value, max_value = con.execute(
                f"SELECT MIN({column_expr}), MAX({column_expr}) FROM {quote_ident(table)}"
            ).fetchone()

        null_rate = None
        if row_count:
            null_rate = null_count / row_count

        column_records.append(
            {
                "run_id": run_id,
                "dataset_name": dataset_name,
                "table_name": logical_table,
                "column_name": column_name,
                "column_type": column_type,
                "row_count": row_count,
                "null_count": null_count,
                "null_rate": null_rate,
                "distinct_count": distinct_count,
                "min_value": stringify_value(min_value),
                "max_value": stringify_value(max_value),
            }
        )

        if null_rate is not None and null_rate > table_entry["max_null_rate"]:
            table_entry["max_null_rate"] = null_rate
            table_entry["nulliest_column"] = column_name

        if distinct_count > table_entry["max_cardinality"]:
            table_entry["max_cardinality"] = distinct_count
            table_entry["high_cardinality_column"] = column_name

        if is_text_type(column_type) and row_count:
            numeric_count = con.execute(
                f"""
                SELECT COUNT(*)
                FROM {quote_ident(table)}
                WHERE {column_expr} IS NOT NULL
                  AND REGEXP_MATCHES(TRIM({column_expr}), '^[+-]?\\d+(\\.\\d+)?$')
                """
            ).fetchone()[0]
            non_numeric = row_count - null_count - numeric_count
            if numeric_count and non_numeric:
                type_issue_records.append(
                    {
                        "run_id": run_id,
                        "dataset_name": dataset_name,
                        "table_name": logical_table,
                        "column_name": column_name,
                        "issue": (
                            f"Mixed numeric/text values ({non_numeric} of {row_count - null_count} "
                            "non-numeric rows)."
                        ),
                        "details": {
                            "numeric_values": numeric_count,
                            "non_numeric_values": non_numeric,
                        },
                    }
                )

        top_rows = con.execute(
            f"""
            SELECT {column_expr} AS value, COUNT(*) AS frequency
            FROM {quote_ident(table)}
            GROUP BY {column_expr}
            ORDER BY frequency DESC, value NULLS LAST
            LIMIT {top_n}
            """
        ).fetchall()
        for rank, (value, frequency) in enumerate(top_rows, start=1):
            top_value_records.append(
                {
                    "run_id": run_id,
                    "dataset_name": dataset_name,
                    "table_name": logical_table,
                    "column_name": column_name,
                    "rank": rank,
                    "value": stringify_value(value),
                    "frequency": frequency,
                    "relative_frequency": frequency / row_count if row_count else None,
                }
            )
</file>

<file path="scripts/profile_tables.py">
#!/usr/bin/env python3
"""Entry point that profiles staged tables into Parquet and HTML artifacts."""

from __future__ import annotations

import argparse
import json
import sys
from datetime import datetime, timezone
from html import escape
from pathlib import Path
from typing import Any

import pandas as pd

REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from scripts.profile_collector import ProfileCollector, ProfileConfig
from scripts.profile_utils import load_json, resolve_stage_path

PROFILES_BASE = REPO_ROOT / "data" / "marts" / "profiles"
REPORTS_RUNS = REPO_ROOT / "reports" / "runs"
DEFAULT_TOP_N = 5


def render_html(
    run_id: str,
    dataset_name: str,
    seed: int | None,
    metadata: dict[str, Any],
    table_aggregates: dict[str, dict[str, Any]],
    type_issues: list[dict[str, Any]],
) -> str:
    title = f"Profile report: {escape(run_id)}"
    lines = [
        "<!doctype html>",
        "<html lang=\"en\">",
        "<head><meta charset=\"utf-8\"><title>Profile report</title></head>",
        "<body>",
        f"<h1>{title}</h1>",
        (
            "<p>Dataset: "
            f"{escape(dataset_name)}"
            + (f" (seed {seed})" if seed is not None else "")
            + f" · profiled at {escape(metadata['profiled_at'])}</p>"
        ),
        "<section>",
        "<h2>Table summary</h2>",
        "<table border=1 cellpadding=4 cellspacing=0>",
        "<tr><th>Table</th><th>Rows</th><th>Columns</th><th>Nulliest column</th>"
        "<th>Max null %</th><th>High cardinality column</th><th>Distinct values</th></tr>",
    ]
    for agg in table_aggregates.values():
        lines.append(
            "<tr>"
            f"<td>{escape(agg['table_name'])}</td>"
            f"<td>{agg['row_count']}</td>"
            f"<td>{agg['column_count']}</td>"
            f"<td>{escape(agg['nulliest_column'] or '')}</td>"
            f"<td>{agg['max_null_rate']:.1%}</td>"
            f"<td>{escape(agg['high_cardinality_column'] or '')}</td>"
            f"<td>{agg['max_cardinality']}</td>"
            "</tr>"
        )
    lines.extend(["</table>", "</section>", "<section>", "<h2>Type issues</h2>"])
    if not type_issues:
        lines.append("<p>No type issues detected.</p>")
    else:
        lines.append("<ul>")
        for issue in type_issues:
            lines.append(
                "<li>"
                f"{escape(issue['table_name'])}.{escape(issue['column_name'])}: "
                f"{escape(issue['issue'])}"
                "</li>"
            )
        lines.append("</ul>")
    lines.append("</section>")
    lines.append("</body></html>")
    return "\n".join(lines)


def main() -> None:
    parser = argparse.ArgumentParser(description="Profile staged tables and persist metadata.")
    parser.add_argument("--dataset-name", default="phase1", help="Name of the ingested dataset.")
    parser.add_argument("--seed", type=int, default=42, help="Seed used when ingesting the data.")
    parser.add_argument(
        "--stage-path",
        type=Path,
        help="Direct path to a staging directory that already exists.",
    )
    parser.add_argument(
        "--top-n",
        type=int,
        default=DEFAULT_TOP_N,
        help="Number of top values to capture per column.",
    )
    args = parser.parse_args()

    stage_path = resolve_stage_path(args.dataset_name, args.seed, args.stage_path)
    if not stage_path.exists():
        raise SystemExit(f"Staging directory {stage_path} does not exist.")
    duckdb_path = stage_path / "staging.duckdb"
    if not duckdb_path.exists():
        raise SystemExit(f"Missing DuckDB file at {duckdb_path}.")

    run_metadata = load_json(stage_path / "run_metadata.json")
    run_id = run_metadata.get("run_id") or f"{args.dataset_name}-{args.seed}-{int(datetime.now(timezone.utc).timestamp())}"
    profile_timestamp = datetime.now(timezone.utc).isoformat()

    profile_dir = PROFILES_BASE / f"run_id={run_id}"
    report_dir = REPORTS_RUNS / run_id
    profile_dir.mkdir(parents=True, exist_ok=True)
    report_dir.mkdir(parents=True, exist_ok=True)

    config = ProfileConfig(
        dataset_name=args.dataset_name,
        seed=args.seed,
        stage_path=stage_path,
        duckdb_path=duckdb_path,
        run_metadata=run_metadata,
        run_id=run_id,
        top_n=args.top_n,
    )
    collector = ProfileCollector(config)
    result = collector.collect()

    metadata = {
        "run_id": run_id,
        "dataset_name": args.dataset_name,
        "seed": args.seed,
        "profiled_at": profile_timestamp,
        "stage_path": str(stage_path),
        "duckdb_path": str(duckdb_path),
        "raw_metadata": run_metadata,
    }

    pd.DataFrame(result.column_records).to_parquet(
        profile_dir / "column_profiles.parquet", index=False
    )
    pd.DataFrame(result.top_value_records).to_parquet(
        profile_dir / "column_top_values.parquet", index=False
    )
    pd.DataFrame(result.type_issue_records).to_parquet(
        profile_dir / "column_type_issues.parquet", index=False
    )
    pd.DataFrame(result.table_summary_records).to_parquet(
        profile_dir / "table_summaries.parquet", index=False
    )
    pd.DataFrame([metadata]).to_parquet(profile_dir / "profile_metadata.parquet", index=False)
    (profile_dir / "profile_metadata.json").write_text(json.dumps(metadata, indent=2))

    html_report = render_html(run_id, args.dataset_name, args.seed, metadata, result.table_aggregates, result.type_issue_records)
    report_path = report_dir / "profile.html"
    report_path.write_text(html_report, encoding="utf-8")

    print(f"Profile artifacts persisted to {profile_dir}.")
    print(f"HTML report available at {report_path}.")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/profile_utils.py">
"""Utility helpers for profile scripts."""

from __future__ import annotations

import json
from datetime import datetime
from pathlib import Path
from typing import Any

import duckdb

STAGING_BASE = Path(__file__).resolve().parents[1] / "data" / "staging"
NUMERIC_KEYWORDS = ("INT", "FLOAT", "DOUBLE", "REAL", "NUMERIC", "DECIMAL", "MONEY")
TEXT_KEYWORDS = ("CHAR", "CLOB", "TEXT")
TEMPORAL_KEYWORDS = ("TIMESTAMP", "DATETIME", "DATE", "TIME")


def quote_ident(value: str) -> str:
    return '"' + value.replace('"', '""') + '"'


def quote_literal(value: str) -> str:
    return "'" + value.replace("'", "''") + "'"


def stringify_value(value: Any) -> Any:
    if value is None:
        return None
    if isinstance(value, datetime):
        return value.isoformat()
    return str(value)


def resolve_stage_path(dataset: str, seed: int, override: Path | None) -> Path:
    if override:
        return override
    return STAGING_BASE / dataset / str(seed)


def load_json(path: Path) -> dict[str, Any]:
    if not path.exists():
        return {}
    return json.loads(path.read_text())


def collect_tables(con: duckdb.DuckDBPyConnection) -> list[str]:
    rows = con.execute(
        """
        SELECT table_name
        FROM information_schema.tables
        WHERE table_schema = 'main'
          AND table_type = 'BASE TABLE'
          AND table_name LIKE 'staging_%'
        ORDER BY table_name
        """
    ).fetchall()
    return [row[0] for row in rows]


def is_numeric_type(type_name: str) -> bool:
    return any(keyword in type_name.upper() for keyword in NUMERIC_KEYWORDS)


def is_text_type(type_name: str) -> bool:
    return any(keyword in type_name.upper() for keyword in TEXT_KEYWORDS)


def is_temporal_type(type_name: str) -> bool:
    return any(keyword in type_name.upper() for keyword in TEMPORAL_KEYWORDS)


Record = dict[str, Any]
</file>

<file path="scripts/publish.py">
#!/usr/bin/env python3
"""Render the DQ Scorecard HTML site from persisted assets."""

from __future__ import annotations

import argparse

try:
try:
    from scripts.publish_helpers import (
        LATEST_REPORT_DIR,
        copy_to_run_directory,
        load_score_payload,
        mutate_context,
        render_scorecard,
    )
except ModuleNotFoundError:  # running directly from scripts/ when tests or tools adjust PYTHONPATH
    from publish_helpers import (
        LATEST_REPORT_DIR,
        copy_to_run_directory,
        load_score_payload,
        mutate_context,
        render_scorecard,
    )
except ModuleNotFoundError:  # running directly from scripts/ when tests or tools adjust PYTHONPATH
    from publish_helpers import (
        LATEST_REPORT_DIR,
        copy_to_run_directory,
        load_score_payload,
        mutate_context,
        render_scorecard,
    )


def main() -> None:
    parser = argparse.ArgumentParser(description="Render the latest DQ scorecard website.")
    parser.add_argument(
        "--run-id",
        help="Optional override for the run identifier used when copying to reports/runs.",
    )
    args = parser.parse_args()

    LATEST_REPORT_DIR.mkdir(parents=True, exist_ok=True)
    score_path = LATEST_REPORT_DIR / "score.json"
    payload = load_score_payload(score_path)
    context = mutate_context(payload)
    html = render_scorecard(context)
    (LATEST_REPORT_DIR / "index.html").write_text(html, encoding="utf-8")

    run_identifier = args.run_id or payload.get("run_id")
    if run_identifier:
        copy_to_run_directory(run_identifier)

    display_id = run_identifier or "latest"
    print(f"Scorecard rendered to {LATEST_REPORT_DIR / 'index.html'} for run {display_id}.")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/quality_gate.py">
#!/usr/bin/env python3
"""Fail the build when the overall score or critical checks miss the target."""

from __future__ import annotations

import argparse
import json
from pathlib import Path
import yaml

import pandas as pd

REPO_ROOT = Path(__file__).resolve().parents[1]
SCORE_JSON_PATH = REPO_ROOT / "reports" / "latest" / "score.json"
CHECK_RESULTS_BASE = REPO_ROOT / "data" / "marts" / "dq_check_results"
CONFIG_PATH = REPO_ROOT / "dq" / "config" / "quality_gate.yml"

DEFAULT_SCORE_THRESHOLD = 90.0
DEFAULT_CRITICAL_SEVERITY = 5


def load_quality_config() -> dict[str, float]:
    if not CONFIG_PATH.exists():
        return {
            "score_threshold": DEFAULT_SCORE_THRESHOLD,
            "critical_severity": DEFAULT_CRITICAL_SEVERITY,
        }
    raw = yaml.safe_load(CONFIG_PATH.read_text()) or {}
    return {
        "score_threshold": float(raw.get("score_threshold", DEFAULT_SCORE_THRESHOLD)),
        "critical_severity": float(raw.get("critical_severity", DEFAULT_CRITICAL_SEVERITY)),
    }


def main() -> None:
    parser = argparse.ArgumentParser(description="Enforce the DQSentry quality gate.")
    parser.add_argument(
        "--score-path",
        type=Path,
        default=SCORE_JSON_PATH,
        help="Score payload used to evaluate the gate.",
    )
    parser.add_argument(
        "--run-id",
        help="Optional run identifier if the score payload does not include one.",
    )
    parser.add_argument(
        "--score-threshold",
        type=float,
        help="Override the configured minimum overall score.",
    )
    parser.add_argument(
        "--critical-severity",
        type=float,
        help="Override the configured severity that defines critical checks.",
    )
    args = parser.parse_args()

    if not args.score_path.exists():
        raise SystemExit(f"Missing score payload at {args.score_path}")
    score_payload = json.loads(args.score_path.read_text())
    score_threshold = (
        args.score_threshold
        if args.score_threshold is not None
        else load_quality_config()["score_threshold"]
    )
    critical_severity = (
        args.critical_severity
        if args.critical_severity is not None
        else load_quality_config()["critical_severity"]
    )

    overall = float(score_payload.get("score", 0.0))
    run_id = args.run_id or score_payload.get("run_id")
    dataset_name = score_payload.get("dataset_name", "unknown")

    if overall < score_threshold:
        raise SystemExit(
            f"Quality gate failed: overall score {overall:.2f} is below threshold {score_threshold:.2f}."
        )

    if not run_id:
        raise SystemExit("Quality gate requires a run_id but none was provided.")

    check_path = CHECK_RESULTS_BASE / f"run_id={run_id}" / "check_results.parquet"
    if not check_path.exists():
        raise SystemExit(f"Missing check results at {check_path}")
    checks = pd.read_parquet(check_path)
    critical_fails = checks[
        (checks["status"] == "fail") & (checks["severity"] >= critical_severity)
    ]

    if not critical_fails.empty:
        checks = sorted(
            f"{row.check_id} ({row.table_name})"
            for row in critical_fails.sort_values("check_id").itertuples(index=False)
        )
        raise SystemExit(
            f"Critical checks failed for run {run_id}: {', '.join(checks)} "
            f"(severity ≥ {critical_severity:.1f})."
        )

    print(
        f"Quality gate passed for run {run_id} ({dataset_name}) "
        f"with overall score {overall:.2f} (threshold {score_threshold:.2f})."
    )


if __name__ == "__main__":
    main()
</file>

<file path="scripts/regression.py">
"""Golden dataset regression suite for locking scoring logic."""

from __future__ import annotations

import argparse
import json
import zipfile
from pathlib import Path
from tempfile import TemporaryDirectory
from typing import Any

import pandas as pd

from scripts.ingest_lib import ingest_dataset
from dq.validate.runner import ValidationRunner

REPO_ROOT = Path(__file__).resolve().parents[1]
DEFAULT_DATASET_ARCHIVE = REPO_ROOT / "dq" / "regression" / "golden_dataset.zip"
DEFAULT_EXPECTED_PATH = REPO_ROOT / "dq" / "regression" / "golden_expected.json"
DEFAULT_DATASET_NAME = "phase1"
DEFAULT_SEED = 42
DEFAULT_TOLERANCE = 0.01


def main() -> None:
    parser = argparse.ArgumentParser(description="Golden dataset regression suite.")
    parser.add_argument(
        "--dataset-archive",
        type=Path,
        default=DEFAULT_DATASET_ARCHIVE,
        help="ZIP archive containing the known-good CSV exports.",
    )
    parser.add_argument(
        "--expected-path",
        type=Path,
        default=DEFAULT_EXPECTED_PATH,
        help="JSON file that holds the expected scores and counts.",
    )
    parser.add_argument(
        "--dataset-name",
        default=DEFAULT_DATASET_NAME,
        help="Logical dataset name to pass through the pipeline.",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=DEFAULT_SEED,
        help="Seed used when ingesting the dataset for deterministic metadata.",
    )
    parser.add_argument(
        "--tolerance",
        type=float,
        default=DEFAULT_TOLERANCE,
        help="Allowed deviation when comparing scores and subscores.",
    )
    parser.add_argument(
        "--update-expected",
        action="store_true",
        help="Refresh the expected output with the current run's results.",
    )
    args = parser.parse_args()

    if not args.dataset_archive.exists():
        raise SystemExit(f"Dataset archive not found at {args.dataset_archive}.")

    actual = _run_golden_pipeline(args.dataset_archive, args.dataset_name, args.seed)

    if args.update_expected:
        _write_expected(actual, args.expected_path)
        print(f"Updated expected regression data at {args.expected_path}.")
        return

    expected = _load_expected(args.expected_path)
    _assert_matches(actual, expected, args.tolerance)
    print(
        "Golden dataset regression passed for run "
        f"{actual['run_id']} (score {actual['score']:.2f})."
    )


def _run_golden_pipeline(
    archive: Path, dataset_name: str, seed: int
) -> dict[str, Any]:
    with TemporaryDirectory(prefix="dq-regression-") as tmpdir:
        temp_root = Path(tmpdir)
        raw_dir = temp_root / "raw"
        raw_dir.mkdir()
        _extract_archive(archive, raw_dir)
        metadata = _load_metadata(raw_dir)
        run_id = metadata.get("run_id")
        logical_dataset_name = metadata.get("dataset_name") or dataset_name
        stage_dir = temp_root / "stage"
        ingest_paths = ingest_dataset(
            dataset_name=logical_dataset_name,
            seed=seed,
            force=True,
            raw_path=raw_dir,
            stage_path=stage_dir,
            run_id=run_id,
        )
        stage_path = Path(ingest_paths["stage_path"])
        duckdb_path = Path(ingest_paths["db_path"])
        runner = ValidationRunner(
            logical_dataset_name,
            ingest_paths["run_id"],
            stage_path,
            duckdb_path,
        )
        summary = runner.run()
        issue_df = _load_issue_log(summary.issue_log_path)
        issue_counts = (
            issue_df["issue_type"].dropna().value_counts().to_dict()
            if not issue_df.empty
            else {}
        )
        subscores = {k: round(float(v), 2) for k, v in (summary.subscores or {}).items()}
        return {
            "run_id": summary.run_id,
            "dataset_name": summary.dataset_name,
            "score": round(summary.score, 2),
            "failed_checks": int(len(issue_df)),
            "issue_counts": {k: int(v) for k, v in issue_counts.items()},
            "subscores": subscores,
        }


def _extract_archive(archive: Path, target: Path) -> None:
    with zipfile.ZipFile(archive, "r") as handle:
        handle.extractall(target)


def _load_metadata(raw_dir: Path) -> dict[str, Any]:
    metadata_path = raw_dir / "run_metadata.json"
    if not metadata_path.exists():
        return {}
    return json.loads(metadata_path.read_text())


def _load_issue_log(path: Path) -> pd.DataFrame:
    if not path.exists():
        return pd.DataFrame()
    return pd.read_parquet(path)


def _load_expected(path: Path) -> dict[str, Any]:
    if not path.exists():
        raise SystemExit(f"Expected regression file missing at {path}.")
    return json.loads(path.read_text())


def _write_expected(actual: dict[str, Any], path: Path) -> None:
    payload = {
        "score": actual["score"],
        "failed_checks": actual["failed_checks"],
        "issue_counts": actual["issue_counts"],
        "subscores": actual["subscores"],
    }
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(payload, indent=2), encoding="utf-8")


def _assert_matches(actual: dict[str, Any], expected: dict[str, Any], tolerance: float) -> None:
    if abs(actual["score"] - expected.get("score", 0.0)) > tolerance:
        raise SystemExit(
            f"Score mismatch: expected {expected.get('score')} vs actual {actual['score']}"
        )
    if actual["failed_checks"] != expected.get("failed_checks", -1):
        raise SystemExit(
            f"Failed-check count mismatch: expected {expected.get('failed_checks')} vs actual {actual['failed_checks']}"
        )
    expected_counts: dict[str, int] = expected.get("issue_counts", {})
    if actual["issue_counts"] != expected_counts:
        raise SystemExit(
            "Issue count mismatch: expected {} vs actual {}".format(
                expected_counts, actual["issue_counts"]
            )
        )
    expected_subscores: dict[str, float] = expected.get("subscores", {})
    missing = set(expected_subscores) - set(actual["subscores"])
    if missing:
        raise SystemExit(f"Missing subscores: {','.join(sorted(missing))}")
    for dimension, target in expected_subscores.items():
        actual_value = actual["subscores"].get(dimension)
        if actual_value is None:
            raise SystemExit(f"Subscore missing for {dimension}.")
        if abs(actual_value - target) > tolerance:
            raise SystemExit(
                f"Subscore mismatch for {dimension}: expected {target} vs actual {actual_value}"
            )


if __name__ == "__main__":
    main()
</file>

<file path="scripts/score_helpers.py">
"""Shared helpers for `scripts/score.py`."""

from __future__ import annotations

from datetime import datetime
from pathlib import Path
from typing import Any

import pandas as pd

REPO_ROOT = Path(__file__).resolve().parents[1]
DATA_MARTS_BASE = REPO_ROOT / "data" / "marts"
CHECK_RESULTS_BASE = DATA_MARTS_BASE / "dq_check_results"
ISSUE_LOG_BASE = DATA_MARTS_BASE / "dq_issue_log"
RUN_HISTORY_PATH = DATA_MARTS_BASE / "dq_run_history" / "run_history.parquet"
SCORE_HISTORY_BASE = DATA_MARTS_BASE / "score_history"
LATEST_REPORT_DIR = REPO_ROOT / "reports" / "latest"
ISSUE_PREVIEW_SIZE = 12
ISSUE_LOG_COLUMNS = [
    "run_id",
    "run_ts",
    "dataset_name",
    "table_name",
    "check_name",
    "dimension",
    "issue_type",
    "severity",
    "affected_rows",
    "affected_pct",
    "sample_bad_rows_json",
    "probable_root_cause",
    "recommended_fix",
    "root_cause_candidates",
]


def parse_iso_timestamp(value: str | None) -> datetime | None:
    if not value:
        return None
    try:
        return datetime.fromisoformat(value)
    except ValueError:
        if value.endswith("Z"):
            try:
                return datetime.fromisoformat(value.replace("Z", "+00:00"))
            except ValueError:
                return None
        return None


def read_run_history(run_id: str) -> tuple[str | None, str | None]:
    if not RUN_HISTORY_PATH.exists():
        return None, None
    history = pd.read_parquet(RUN_HISTORY_PATH)
    if history.empty:
        return None, None
    match = history[history["run_id"] == run_id]
    if match.empty:
        return None, None
    record = match.iloc[-1]
    return record.get("run_ts"), record.get("dataset_name")


def format_record(value: Any) -> Any:
    if pd.isna(value):
        return "" if isinstance(value, str) else None
    if isinstance(value, (float, int, str, bool)):
        return value
    return str(value)


def build_check_summary(checks: pd.DataFrame) -> list[dict[str, Any]]:
    columns = [
        "check_id",
        "table_name",
        "dimension",
        "description",
        "status",
        "failure_rate",
        "threshold_warning",
        "threshold_fail",
        "severity",
        "weight",
        "penalty",
        "issue_type",
    ]
    ordered = checks.sort_values(["status", "failure_rate"], ascending=[True, False])
    records: list[dict[str, Any]] = []
    for _, row in ordered.iterrows():
        entry = {key: format_record(row.get(key)) for key in columns}
        entry["failure_rate"] = float(entry["failure_rate"] or 0.0)
        entry["threshold_warning"] = float(entry["threshold_warning"] or 0.0)
        entry["threshold_fail"] = float(entry["threshold_fail"] or 0.0)
        entry["severity"] = int(entry["severity"] or 0)
        entry["weight"] = float(entry["weight"] or 0.0)
        entry["penalty"] = float(entry["penalty"] or 0.0)
        records.append(entry)
    return records


def build_issue_preview(issues: pd.DataFrame) -> list[dict[str, Any]]:
    if issues.empty:
        return []
    ordered = issues.sort_values(
        ["severity", "affected_pct"], ascending=[False, False]
    ).head(ISSUE_PREVIEW_SIZE)
    preview: list[dict[str, Any]] = []
    for _, row in ordered.iterrows():
        preview.append(
            {
                "run_id": row.get("run_id"),
                "table_name": row.get("table_name"),
                "check_name": row.get("check_name"),
                "issue_type": row.get("issue_type"),
                "severity": int(row.get("severity") or 0),
                "affected_pct": float(row.get("affected_pct") or 0.0),
                "probable_root_cause": row.get("probable_root_cause") or "",
                "recommended_fix": row.get("recommended_fix") or "",
            }
        )
    return preview


def append_score_history(record: dict[str, Any]) -> None:
    SCORE_HISTORY_BASE.mkdir(parents=True, exist_ok=True)
    history_path = SCORE_HISTORY_BASE / "score_history.parquet"
    frame = pd.DataFrame([record])
    if history_path.exists():
        existing = pd.read_parquet(history_path)
        frame = pd.concat([existing, frame], ignore_index=True)
    frame.to_parquet(history_path, index=False)
    run_dir = SCORE_HISTORY_BASE / f"run_id={record['run_id']}"
    run_dir.mkdir(parents=True, exist_ok=True)
    frame.iloc[-1:].to_parquet(run_dir / "score_summary.parquet", index=False)


def compute_scores_from_checks(
    checks: pd.DataFrame, baseline: float, minimum: float
) -> tuple[float, dict[str, float]]:
    total_weight = float(checks["weight"].sum())
    total_penalty = float(checks["penalty"].sum())
    normalized = total_penalty / total_weight if total_weight else 0.0
    score = max(minimum, baseline - 100 * normalized)
    dims: dict[str, float] = {}
    grouped = checks.groupby("dimension", as_index=True).agg(
        penalty=("penalty", "sum"), weight=("weight", "sum")
    )
    for dimension, row in grouped.iterrows():
        weight = float(row["weight"])
        penalty = float(row["penalty"])
        if not weight:
            dims[dimension] = float(baseline)
            continue
        dims[dimension] = max(minimum, baseline - 100 * (penalty / weight))
    return score, dims
</file>

<file path="scripts/score.py">
#!/usr/bin/env python3
"""Compute scores from check results and generate latest scorecard data."""

from __future__ import annotations

import argparse
import json
import sys
from datetime import datetime, timezone
from pathlib import Path

import pandas as pd

REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from dq.validate.config import load_rules

from scripts.score_helpers import (
    CHECK_RESULTS_BASE,
    ISSUE_LOG_BASE,
    ISSUE_LOG_COLUMNS,
    LATEST_REPORT_DIR,
    append_score_history,
    build_check_summary,
    build_issue_preview,
    compute_scores_from_checks,
    parse_iso_timestamp,
    read_run_history,
)

RULES_PATH = REPO_ROOT / "dq" / "config" / "rules.yml"


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Compute run-level scores and persist scorecard artifacts."
    )
    parser.add_argument("--run-id", required=True, help="Run identifier to publish.")
    args = parser.parse_args()

    run_id = args.run_id
    checks_path = CHECK_RESULTS_BASE / f"run_id={run_id}" / "check_results.parquet"
    if not checks_path.exists():
        raise SystemExit(f"Missing check results at {checks_path}")
    issues_path = ISSUE_LOG_BASE / f"run_id={run_id}" / "issue_log.parquet"
    checks = pd.read_parquet(checks_path)
    issues = pd.DataFrame()
    if issues_path.exists():
        issues = pd.read_parquet(issues_path)

    _, baseline_cfg, minimum_cfg = load_rules(RULES_PATH)
    baseline = baseline_cfg
    minimum = minimum_cfg

    run_ts_value, dataset_hint = read_run_history(run_id)
    if not issues.empty:
        available_ts = issues["run_ts"].dropna()
        issue_ts = available_ts.iat[0] if not available_ts.empty else None
        if not run_ts_value and issue_ts:
            run_ts_value = issue_ts
    run_ts = parse_iso_timestamp(run_ts_value) or datetime.now(timezone.utc)
    dataset_name = checks["dataset_name"].iat[0] if not checks.empty else dataset_hint or "unknown"

    overall_score, subscores = compute_scores_from_checks(checks, baseline, minimum)
    issue_preview = build_issue_preview(issues)
    check_summary = build_check_summary(checks)
    issue_counts = (
        issues["issue_type"].value_counts().to_dict()
        if not issues.empty
        else {}
    )

    if run_ts.tzinfo is None:
        run_ts = run_ts.replace(tzinfo=timezone.utc)
    run_ts_iso = run_ts.astimezone(timezone.utc).isoformat()

    history_record = {
        "run_id": run_id,
        "run_ts": run_ts_iso,
        "dataset_name": dataset_name,
        "score": overall_score,
        "baseline": baseline,
        "minimum": minimum,
        "total_penalty": float(checks["penalty"].sum()),
        "total_weight": float(checks["weight"].sum()),
        "total_checks": int(len(checks)),
        "failed_checks": int(len(issues)),
        "subscores": json.dumps({k: float(v) for k, v in subscores.items()}, ensure_ascii=False),
    }
    append_score_history(history_record)

    payload = {
        "run_id": run_id,
        "run_ts": run_ts_iso,
        "dataset_name": dataset_name,
        "score": round(overall_score, 2),
        "baseline": baseline,
        "minimum": minimum,
        "total_checks": int(len(checks)),
        "failed_checks": int(len(issues)),
        "issue_counts": {k: int(v) for k, v in issue_counts.items()},
        "subscores": {k: round(v, 2) for k, v in subscores.items()},
        "check_summary": check_summary,
        "issue_preview": issue_preview,
        "generated_at": datetime.now(timezone.utc).isoformat(),
    }

    LATEST_REPORT_DIR.mkdir(parents=True, exist_ok=True)
    score_json_path = LATEST_REPORT_DIR / "score.json"
    score_json_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")

    issues_csv_path = LATEST_REPORT_DIR / "issues.csv"
    if not issues.empty:
        issues.to_csv(issues_csv_path, index=False)
    else:
        pd.DataFrame(columns=ISSUE_LOG_COLUMNS).to_csv(issues_csv_path, index=False)

    print(f"Score data written to {score_json_path}")
    print(f"Issue log (CSV) written to {issues_csv_path}")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/validate_runner.py">
#!/usr/bin/env python3
"""CLI that drives the DQSentry validation runner."""

from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path

REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from dq.validate.runner import ValidationRunner
from scripts.profile_utils import resolve_stage_path


def load_run_metadata(stage_path: Path) -> dict[str, str]:
    metadata_path = stage_path / "run_metadata.json"
    if not metadata_path.exists():
        raise SystemExit(f"Run metadata missing at {metadata_path}")
    return json.loads(metadata_path.read_text())


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Execute validation checks for a staged dataset."
    )
    parser.add_argument("--dataset-name", help="Dataset name (default comes from staging metadata).")
    parser.add_argument("--seed", type=int, default=42, help="Seed used to ingest the dataset.")
    parser.add_argument(
        "--stage-path",
        type=Path,
        help="Path to an already ingested staging directory (overrides dataset/seed).",
    )
    parser.add_argument("--run-id", help="Optional override for the run identifier.")
    parser.add_argument(
        "--duckdb-path",
        type=Path,
        help="Optional DuckDB file path (defaults to staging/staging.duckdb).",
    )
    args = parser.parse_args()

    stage_path = args.stage_path or resolve_stage_path(args.dataset_name or "phase1", args.seed, None)
    if not stage_path.exists():
        raise SystemExit(f"Staging directory not found at {stage_path}")

    metadata = load_run_metadata(stage_path)
    dataset_name = args.dataset_name or metadata.get("dataset_name")
    if not dataset_name:
        raise SystemExit("Unable to determine dataset name for validation.")
    run_id = args.run_id or metadata.get("run_id")
    if not run_id:
        raise SystemExit("Run identifier is required to execute validation.")

    duckdb_path = args.duckdb_path or stage_path / "staging.duckdb"
    if not duckdb_path.exists():
        raise SystemExit(f"DuckDB file missing at {duckdb_path}")

    runner = ValidationRunner(dataset_name, run_id, stage_path, duckdb_path)
    summary = runner.run()

    print(
        f"Validation complete · run_id={summary.run_id} · dataset={summary.dataset_name} "
        f"· score={summary.score:.2f}"
    )


if __name__ == "__main__":
    main()
</file>

<file path="tools/generate_synthetic.py">
#!/usr/bin/env python3
"""Command that delegates to the split synthetic-generator modules."""

import argparse

try:
    from tools.synthetic_cli import generate_dataset
except ModuleNotFoundError:
    from synthetic_cli import generate_dataset


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Generate deterministic synthetic exports with deliberate issues."
    )
    parser.add_argument(
        "--dataset-name", default="phase1", help="Subdirectory under data/raw to populate."
    )
    parser.add_argument("--seed", type=int, default=42, help="Deterministic run seed.")
    parser.add_argument("--force", action="store_true", help="Overwrite an existing run directory.")
    args = parser.parse_args()
    generate_dataset(args.dataset_name, args.seed, args.force)


if __name__ == "__main__":
    main()
</file>

<file path="tools/synthetic_builder.py">
"""Helpers for building each table of the synthetic exports."""

import random
import uuid

NS = uuid.UUID("3c2f0d46-5df0-4c88-9a5d-dcbf6f3d43e9")

try:
    from tools.synthetic_templates import (
        DISTRICT_TEMPLATES,
        EVENT_TEMPLATES,
        NEWSLETTER_TEMPLATES,
        RESOURCE_TEMPLATES,
        USER_TEMPLATES,
    )
except ModuleNotFoundError:
    from synthetic_templates import (
        DISTRICT_TEMPLATES,
        EVENT_TEMPLATES,
        NEWSLETTER_TEMPLATES,
        RESOURCE_TEMPLATES,
        USER_TEMPLATES,
    )


def dataset_offset(dataset_name: str) -> int:
    return sum(ord(ch) for ch in (dataset_name or ""))


def stable_id(kind: str, identifier: str, dataset_name: str, seed: int) -> str:
    key = f"{dataset_name}-{seed}-{kind}-{identifier}"
    return str(uuid.uuid5(NS, key))


def shuffle_rows(rows: list[dict], dataset_name: str, seed: int, offset: int) -> list[dict]:
    rng = random.Random(seed + dataset_offset(dataset_name) + offset)
    shuffled = list(rows)
    rng.shuffle(shuffled)
    return shuffled


def build_districts(dataset_name: str, seed: int) -> list[dict]:
    rows = [
        {
            "district_id": stable_id("district", template["key"], dataset_name, seed),
            "district_name": template["name"],
            "state": template["state"],
        }
        for template in DISTRICT_TEMPLATES
    ]
    return shuffle_rows(rows, dataset_name, seed, offset=10)


def build_users(
    dataset_name: str, seed: int, district_ids: dict[str, str]
) -> list[dict]:
    org_names = {
        "Mastery Labs": "org-mastery",
        "NYC Elevate": "org-nyc",
        "Northside Alliance": "org-northside",
        "Sunrise Initiative": "org-sunrise",
    }
    rows = []
    for template in USER_TEMPLATES:
        org_id = stable_id("org", org_names[template["org"]], dataset_name, seed)
        user_id = stable_id("user", template["key"], dataset_name, seed)
        district_key = template["district"]
        district_id = district_ids.get(district_key) if district_key else ""
        rows.append(
            {
                "user_id": user_id,
                "email": template["email"],
                "org_id": org_id,
                "role": template["role"],
                "state": template["state"],
                "district_id": district_id,
            }
        )
    return shuffle_rows(rows, dataset_name, seed, offset=20)


def build_resources(dataset_name: str, seed: int) -> list[dict]:
    rows = []
    for template in RESOURCE_TEMPLATES:
        resource_id = stable_id("resource", template["key"], dataset_name, seed)
        rows.append(
            {
                "resource_id": resource_id,
                "type": template["type"],
                "subject": template["subject"],
                "grade_band": template["grade_band"],
            }
        )
    return shuffle_rows(rows, dataset_name, seed, offset=30)


def build_events(
    dataset_name: str,
    seed: int,
    user_ids: dict[str, str],
    resource_ids: dict[str, str],
) -> list[dict]:
    rows = []
    for template in EVENT_TEMPLATES:
        event_id = stable_id("event", template["key"], dataset_name, seed)
        rows.append(
            {
                "event_id": event_id,
                "user_id": user_ids[template["user"]],
                "resource_id": resource_ids.get(
                    template["resource"],
                    stable_id("resource", template["resource"], dataset_name, seed),
                ),
                "event_type": template["event_type"],
                "event_ts": template["event_ts"],
            }
        )
    return shuffle_rows(rows, dataset_name, seed, offset=40)


def build_newsletter(dataset_name: str, seed: int) -> list[dict]:
    return shuffle_rows(
        [
            {
                "email": template["email"],
                "subscribed_at": template["subscribed_at"],
                "opened_at": template["opened_at"],
                "clicked_at": template["clicked_at"],
            }
            for template in NEWSLETTER_TEMPLATES
        ],
        dataset_name,
        seed,
        offset=50,
    )
</file>

<file path="tools/synthetic_cli.py">
"""Orchestrates CSV writing and metadata exports for the synthetic data generator."""

import csv
import json
import shutil
import textwrap
from datetime import datetime, timezone
from pathlib import Path

try:
    from tools.synthetic_builder import (
        build_districts,
        build_events,
        build_newsletter,
        build_resources,
        build_users,
        stable_id,
    )
except ModuleNotFoundError:
    from synthetic_builder import (
        build_districts,
        build_events,
        build_newsletter,
        build_resources,
        build_users,
        stable_id,
    )

REPO_ROOT = Path(__file__).resolve().parents[1]
RAW_BASE = REPO_ROOT / "data" / "raw"

FIELD_SETS = {
    "districts.csv": ("district_id", "district_name", "state"),
    "users.csv": ("user_id", "email", "org_id", "role", "state", "district_id"),
    "resources.csv": ("resource_id", "type", "subject", "grade_band"),
    "events.csv": ("event_id", "user_id", "resource_id", "event_type", "event_ts"),
    "newsletter.csv": ("email", "subscribed_at", "opened_at", "clicked_at"),
}

DISTRICT_KEYS = ("nyc", "northside", "southvalley", "sunrise")
USER_KEYS = ("alice", "bob", "carol", "dax", "eve", "frank", "glenda")
RESOURCE_KEYS = ("math-fundamentals", "capstone-think", "science-explorers", "pathway-start")


def write_csv(path: Path, columns: tuple[str, ...], rows: list[dict]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", newline="", encoding="utf-8") as handle:
        writer = csv.DictWriter(handle, fieldnames=columns)
        writer.writeheader()
        for row in rows:
            writer.writerow(
                {col: (row.get(col) if row.get(col) is not None else "") for col in columns}
            )


def generate_dataset(dataset_name: str, seed: int, force: bool = False) -> dict:
    raw_path = RAW_BASE / dataset_name / str(seed)
    if raw_path.exists():
        if force:
            shutil.rmtree(raw_path)
        else:
            raise SystemExit(
                f"Run directory {raw_path} already exists. Use --force to overwrite."
            )
    raw_path.mkdir(parents=True, exist_ok=True)

    district_rows = build_districts(dataset_name, seed)
    district_ids = {
        key: stable_id("district", key, dataset_name, seed) for key in DISTRICT_KEYS
    }
    user_rows = build_users(dataset_name, seed, district_ids)
    resource_rows = build_resources(dataset_name, seed)
    user_id_map = {
        key: stable_id("user", key, dataset_name, seed) for key in USER_KEYS
    }
    resource_id_map = {
        key: stable_id("resource", key, dataset_name, seed) for key in RESOURCE_KEYS
    }
    event_rows = build_events(dataset_name, seed, user_id_map, resource_id_map)
    newsletter_rows = build_newsletter(dataset_name, seed)

    data_map = {
        "districts.csv": district_rows,
        "users.csv": user_rows,
        "resources.csv": resource_rows,
        "events.csv": event_rows,
        "newsletter.csv": newsletter_rows,
    }
    for csv_name, rows in data_map.items():
        write_csv(raw_path / csv_name, FIELD_SETS[csv_name], rows)

    metadata = {
        "dataset_name": dataset_name,
        "seed": seed,
        "run_id": stable_id("run", str(seed), dataset_name, seed),
        "generated_at": datetime.now(timezone.utc).isoformat(),
    }
    (raw_path / "run_metadata.json").write_text(json.dumps(metadata, indent=2))

    print(
        textwrap.dedent(
            f"""\
            Generated synthetic dataset "{dataset_name}" with seed {seed}.
            Raw exports written under {raw_path}.
            Run ID: {metadata['run_id']}.
            """
        ).strip()
    )
    return metadata
</file>

<file path="tools/synthetic_templates.py">
"""Defines the raw template rows used by the synthetic generator."""

DISTRICT_TEMPLATES = [
    {"key": "nyc", "name": "NYC", "state": " ny "},
    {"key": "northside", "name": "  northside unified", "state": "Ca"},
    {"key": "northside", "name": "Northside Unified", "state": "CA"},
    {"key": "southvalley", "name": "South Valley", "state": "Texas"},
    {"key": "sunrise", "name": "Sunrise Charter", "state": " CAX "},
]

USER_TEMPLATES = [
    {
        "key": "alice",
        "email": "  Alice.Smith@Example.com ",
        "org": "Mastery Labs",
        "role": "teacher",
        "state": "ca",
        "district": "northside",
    },
    {
        "key": "bob",
        "email": "bob@example.COM",
        "org": "NYC Elevate",
        "role": "district_admin",
        "state": "NY",
        "district": "nyc",
    },
    {
        "key": "carol",
        "email": "",
        "org": "Northside Alliance",
        "role": "student",
        "state": "California",
        "district": "northside",
    },
    {
        "key": "dax",
        "email": " Dax@Learning.org  ",
        "org": "Mastery Labs",
        "role": "teacher",
        "state": "tx",
        "district": "southvalley",
    },
    {
        "key": "eve",
        "email": "EVE.lem@Example.com",
        "org": "Sunrise Initiative",
        "role": "other",
        "state": " cax ",
        "district": None,
    },
    {
        "key": "frank",
        "email": " frank@example.com",
        "org": "Sunrise Initiative",
        "role": "admin",
        "state": "zz",
        "district": "southvalley",
    },
    {
        "key": "glenda",
        "email": "GLENDA@Example.Com",
        "org": "NYC Elevate",
        "role": "teacher",
        "state": "nyc",
        "district": "nyc",
    },
    {
        "key": "alice",
        "email": "ALICE.Smith@example.com",
        "org": "Mastery Labs",
        "role": "teacher",
        "state": "ca",
        "district": "northside",
    },
]

RESOURCE_TEMPLATES = [
    {
        "key": "math-fundamentals",
        "type": "lesson",
        "subject": "  Math ",
        "grade_band": "grades pre k to 2",
    },
    {
        "key": "capstone-think",
        "type": "Assessment",
        "subject": "ELA",
        "grade_band": "Grades 3-5",
    },
    {
        "key": "science-explorers",
        "type": "ACTIVITY",
        "subject": " science",
        "grade_band": "Grades 6-8 ",
    },
    {
        "key": "pathway-start",
        "type": "module ",
        "subject": "Social Studies",
        "grade_band": "High School",
    },
    {
        "key": "math-fundamentals",
        "type": "lesson",
        "subject": "math",
        "grade_band": "PreK-2",
    },
]

EVENT_TEMPLATES = [
    {
        "key": "evt-view",
        "user": "alice",
        "resource": "math-fundamentals",
        "event_type": " view ",
        "event_ts": "2024-01-15 14:30:00",
    },
    {
        "key": "evt-start",
        "user": "bob",
        "resource": "capstone-think",
        "event_type": "Start",
        "event_ts": "01/06/2024 08:00:00",
    },
    {
        "key": "evt-start",
        "user": "bob",
        "resource": "capstone-think",
        "event_type": "start",
        "event_ts": "01/06/2024 08:00 AM",
    },
    {
        "key": "evt-complete",
        "user": "carol",
        "resource": "science-explorers",
        "event_type": "complete",
        "event_ts": "March 3 2024 16:35",
    },
    {
        "key": "evt-orphan",
        "user": "glenda",
        "resource": "missing-resource",
        "event_type": "submit",
        "event_ts": "2024-03-05T09:15:00+00:00",
    },
    {
        "key": "evt-share",
        "user": "frank",
        "resource": "pathway-start",
        "event_type": "share",
        "event_ts": "04/02/2024 13:20",
    },
]

NEWSLETTER_TEMPLATES = [
    {
        "email": "  Alice.Smith@Example.com",
        "subscribed_at": "2024-01-01T09:00:00Z",
        "opened_at": "2024-01-02 10:00 AM",
        "clicked_at": "",
    },
    {
        "email": "bob@example.com",
        "subscribed_at": "1/5/2024 08:00",
        "opened_at": "2024-01-06T09:30:00+00:00",
        "clicked_at": "Jan 7 2024 10:00",
    },
    {
        "email": "",
        "subscribed_at": "2024-01-08 11:11:11",
        "opened_at": "",
        "clicked_at": "",
    },
    {
        "email": "bob@example.com",
        "subscribed_at": "2024-01-05 08:00:00",
        "opened_at": "2024-01-06 09:30",
        "clicked_at": "2024-01-07 10:00",
    },
]
</file>

<file path=".gitignore">
# Python artifacts
__pycache__/
*.py[cod]
*.egg-info/
.venv/

# Data artifacts generated by the pipeline
data/raw/
data/staging/
data/marts/
dq/ge/
reports/latest/
reports/runs/

# Notebook artifacts
.ipynb_checkpoints/

# Logs and temporary files
*.log
*.tmp
</file>

<file path=".github/workflows/dq_push.yml">
name: DQ Push Pipeline

on:
  push: {}
  pull_request: {}

permissions:
  contents: write

env:
  DATASET_NAME: phase1
  SEED: ${{ github.run_number }}
  SKIP_SAMPLE_DATA_GENERATION: false

jobs:
  pipeline:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: requirements.txt

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Generate sample data (optional)
        if: env.SKIP_SAMPLE_DATA_GENERATION != 'true'
        run: |
          python tools/generate_synthetic.py \
            --dataset-name "$DATASET_NAME" \
            --seed "$SEED" \
            --force

      - name: Ingest data
        run: |
          python scripts/ingest.py \
            --dataset-name "$DATASET_NAME" \
            --seed "$SEED" \
            --force

      - name: Validate data
        run: |
          python scripts/validate_runner.py \
            --dataset-name "$DATASET_NAME" \
            --seed "$SEED"

      - name: Score run
        id: score
        shell: bash
        run: |
          STAGE_DIR="data/staging/$DATASET_NAME/$SEED"
          RUN_ID=$(STAGE_DIR="$STAGE_DIR" python - <<'PY'
from pathlib import Path
import json, os
metadata_path = Path(os.environ['STAGE_DIR']) / 'run_metadata.json'
print(json.loads(metadata_path.read_text())['run_id'])
PY
          )
          echo "run_id=$RUN_ID" >> "$GITHUB_OUTPUT"
          python scripts/score.py --run-id "$RUN_ID"

      - name: Golden dataset regression
        run: |
          python -m scripts.regression

      - name: Build scorecard
        run: |
          python scripts/publish.py --run-id "${{ steps.score.outputs.run_id }}"

      - name: Enforce quality gate
        run: |
          python scripts/quality_gate.py --run-id "${{ steps.score.outputs.run_id }}"

      - name: Upload run artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dq-run-${{ steps.score.outputs.run_id }}
          path: |
            reports/latest
            reports/runs/run_id=${{ steps.score.outputs.run_id }}
            data/marts/dq_check_results/run_id=${{ steps.score.outputs.run_id }}
            data/marts/dq_issue_log/run_id=${{ steps.score.outputs.run_id }}

      - name: Publish scorecard to GitHub Pages
        if: success()
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: reports/latest
          publish_branch: gh-pages
          user_name: github-actions[bot]
          user_email: github-actions[bot]@users.noreply.github.com
          commit_message: "chore: publish DQ scorecard run ${{ steps.score.outputs.run_id }}"
          allow_empty_commit: true
</file>

<file path="dq/config/rules.yml">
checks:
  completeness:
    - id: users_email_not_null
      table: users
      column: email
      description: "Email must be populated for every CRM contact."
      threshold:
        warning: 0.02
        fail: 0.05
      severity: 3
      weight: 1.5
      rule: "NULL_PERCENTAGE"
    - id: events_event_ts_not_null
      table: events
      column: event_ts
      description: "Every interaction must record an event timestamp."
      threshold:
        warning: 0.01
        fail: 0.03
      severity: 4
      weight: 1.2
      rule: "NULL_PERCENTAGE"
    - id: users_state_not_null
      table: users
      column: state
      description: "Users must have an associated state code."
      threshold:
        warning: 0.01
        fail: 0.03
      severity: 3
      weight: 1.0
      rule: "NULL_PERCENTAGE"
    - id: users_district_id_not_null
      table: users
      column: district_id
      description: "Each user record should land inside a district reference."
      threshold:
        warning: 0.10
        fail: 0.25
      severity: 2
      weight: 0.7
      rule: "NULL_PERCENTAGE"
    - id: events_resource_id_not_null
      table: events
      column: resource_id
      description: "Event rows need to reference a resource."
      threshold:
        warning: 0.005
        fail: 0.01
      severity: 4
      weight: 1.2
      rule: "NULL_PERCENTAGE"
    - id: resources_resource_id_not_null
      table: resources
      column: resource_id
      description: "Resource catalog entries must carry their identifier."
      threshold:
        warning: 0.001
        fail: 0.01
      severity: 3
      weight: 0.8
      rule: "NULL_PERCENTAGE"
  validity_accuracy:
    - id: users_email_format
      table: users
      column: email
      description: "CRM emails should match a reasonable email pattern."
      threshold:
        warning: 0.01
        fail: 0.03
      severity: 3
      weight: 1.5
      rule: 'PATTERN(^[^\s@]+@[^\s@]+\.[^\s@]+$)'
    - id: users_state_valid
      table: users
      column: state
      description: "State codes for users must follow the USPS standard."
      threshold:
        warning: 0.01
        fail: 0.03
      severity: 3
      weight: 1.0
      rule: "PATTERN(^[A-Z]{2}$)"
    - id: districts_state_valid
      table: districts
      column: state
      description: "District states must use the USPS 2-letter codes."
      threshold:
        warning: 0.01
        fail: 0.03
      severity: 2
      weight: 0.8
      rule: "PATTERN(^[A-Z]{2}$)"
    - id: events_event_ts_date_range
      table: events
      column: event_ts
      description: "Event timestamps must stay inside plausible calendar bounds."
      threshold:
        warning: 0.01
        fail: 0.03
      severity: 3
      weight: 1.1
      rule: "DATE_RANGE(1900-01-01T00:00:00Z, now)"
    - id: newsletter_subscribed_at_date_range
      table: newsletter
      column: subscribed_at
      description: "Newsletter subscription times should fit within the historical window."
      threshold:
        warning: 0.01
        fail: 0.05
      severity: 2
      weight: 0.7
      rule: "DATE_RANGE(2000-01-01T00:00:00Z, now)"
    - id: newsletter_opened_at_date_range
      table: newsletter
      column: opened_at
      description: "Newsletter opened timestamps must not be in the future."
      threshold:
        warning: 0.01
        fail: 0.05
      severity: 2
      weight: 0.6
      rule: "DATE_RANGE(2000-01-01T00:00:00Z, now)"
    - id: newsletter_clicked_at_date_range
      table: newsletter
      column: clicked_at
      description: "Newsletter click timestamps must stay within the plausible window."
      threshold:
        warning: 0.01
        fail: 0.05
      severity: 2
      weight: 0.6
      rule: "DATE_RANGE(2000-01-01T00:00:00Z, now)"
    - id: numeric_counts_non_negative
      table: "*"
      description: "Count-style numeric columns must never go negative."
      threshold:
        warning: 0.0
        fail: 0.0
      severity: 3
      weight: 1.2
      rule: "NON_NEGATIVE_COUNTS"
      column_regex: "(?i).*count.*"
  consistency:
    - id: newsletter_timestamps_order
      table: newsletter
      columns: [subscribed_at, opened_at, clicked_at]
      description: "Tracking timestamps should progress monotonically from subscribe to click."
      threshold:
        warning: 0.05
        fail: 0.1
      severity: 2
      weight: 0.9
      rule: "TIMESTAMP_ORDER"
    - id: resource_grade_band_values
      table: resources
      column: grade_band
      description: "Grade-band values should match the configured taxonomy."
      threshold:
        warning: 0.1
        fail: 0.2
      severity: 2
      weight: 0.7
      rule: "ENUM(PreK-2,3-5,6-8,9-12)"
    - id: resources_type_normalized
      table: resources
      column: type
      description: "Resource types must use the canonical lower-case categories."
      threshold:
        warning: 0.01
        fail: 0.03
      severity: 2
      weight: 0.8
      rule: "ENUM(lesson,assessment,activity,module,pathway)"
    - id: users_role_normalized
      table: users
      column: role
      description: "User roles must stay within the allowed normalized set."
      threshold:
        warning: 0.01
        fail: 0.03
      severity: 2
      weight: 0.8
      rule: "ENUM(student,teacher,admin,district_admin,other)"
    - id: districts_name_to_id_unique
      table: districts
      description: "Each district name should map to a single district_id after canonicalization."
      threshold:
        warning: 0.001
        fail: 0.005
      severity: 3
      weight: 1.0
      rule: "UNIQUE_MAPPING(district_name, district_id)"
  uniqueness:
    - id: users_email_unique
      table: users
      column: email
      description: "Email should uniquely identify CRM records."
      threshold:
        warning: 0.005
        fail: 0.02
      severity: 4
      weight: 1.3
      rule: "DUPLICATE_PERCENTAGE"
    - id: events_pk_unique
      table: events
      column: event_id
      description: "Each event_id must be unique to avoid lost history."
      threshold:
        warning: 0.001
        fail: 0.005
      severity: 5
      weight: 1.8
      rule: "DUPLICATE_PERCENTAGE"
    - id: resources_resource_id_unique
      table: resources
      column: resource_id
      description: "Resource IDs must stay unique inside the catalog."
      threshold:
        warning: 0.001
        fail: 0.005
      severity: 4
      weight: 1.3
      rule: "DUPLICATE_PERCENTAGE"
    - id: users_user_id_unique
      table: users
      column: user_id
      description: "User identifiers must never collide."
      threshold:
        warning: 0.001
        fail: 0.002
      severity: 5
      weight: 1.4
      rule: "DUPLICATE_PERCENTAGE"
    - id: users_email_org_unique
      table: users
      columns: [email, org_id]
      description: "The (email, org) compound key should dedupe duplicates."
      threshold:
        warning: 0.005
        fail: 0.02
      severity: 3
      weight: 1.1
      rule: "DUPLICATE_PERCENTAGE"
  integrity:
    - id: events_user_fk
      table: events
      column: user_id
      description: "Events must reference an existing user record."
      threshold:
        warning: 0.01
        fail: 0.03
      severity: 5
      weight: 2.0
      rule: "FK(users.user_id)"
    - id: events_resource_fk
      table: events
      column: resource_id
      description: "Resources referenced in events must exist in the master catalog."
      threshold:
        warning: 0.01
        fail: 0.03
      severity: 5
      weight: 1.8
      rule: "FK(resources.resource_id)"
    - id: users_district_fk
      table: users
      column: district_id
      description: "Users must belong to a referenceable district."
      threshold:
        warning: 0.01
        fail: 0.05
      severity: 4
      weight: 1.2
      rule: "FK(districts.district_id)"
penalty_formula:
  note: "Each check produces a failure_rate (0–1). Penalty = failure_rate * (severity / 5) * weight."
  normalization: "Sum of all weights defines the denominator when scaling overall penalty back into [0,1]."

score:
  baseline: 100
  min: 0
</file>

<file path="dq/validate/config.py">
"""Helpers for loading rule metadata."""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Tuple

import yaml


@dataclass(frozen=True)
class Threshold:
    warning: float
    fail: float


@dataclass(frozen=True)
class RootCause:
    probable_cause: str
    recommended_fix: str


@dataclass(frozen=True)
class CheckRule:
    id: str
    table: str
    dimension: str
    description: str
    columns: list[str] | None
    column_regex: str | None
    rule_type: str
    rule_args: list[str]
    severity: int
    weight: float
    threshold: Threshold
    metadata: dict[str, Any]
    root_causes: tuple[RootCause, ...]


def _parse_rule(rule_text: str) -> tuple[str, list[str]]:
    rule_text = rule_text.strip()
    if "(" in rule_text and rule_text.endswith(")"):
        name, arg_text = rule_text.split("(", 1)
        args = [arg.strip() for arg in arg_text[:-1].split(",")]
        return name, [arg for arg in args if arg]
    return rule_text, []


def load_rules(path: Path) -> tuple[list[CheckRule], float, float]:
    raw = yaml.safe_load(path.read_text())
    root_cause_map = load_root_causes()
    checks: dict[str, Iterable[dict[str, Any]]] = raw.get("checks") or {}
    rules: list[CheckRule] = []
    for dimension, entries in checks.items():
        for entry in entries or []:
            rule_type, rule_args = _parse_rule(entry["rule"])
            column = entry.get("column")
            columns = entry.get("columns")
            if not columns and column:
                columns = [column]
            threshold_data = entry.get("threshold") or {}
            threshold = Threshold(
                warning=float(threshold_data.get("warning", 0.0)),
                fail=float(threshold_data.get("fail", 0.0)),
            )
            rules.append(
                CheckRule(
                    id=entry["id"],
                    table=entry["table"],
                    dimension=dimension,
                    description=entry.get("description", ""),
                    columns=columns,
                    column_regex=entry.get("column_regex"),
                    rule_type=rule_type,
                    rule_args=rule_args,
                    severity=int(entry.get("severity", 1)),
                    weight=float(entry.get("weight", 1.0)),
                    threshold=threshold,
                    metadata=entry,
                    root_causes=root_cause_map.get(entry["id"], ()),
                )
            )
    score_cfg: dict[str, Any] = raw.get("score") or {}
    baseline = float(score_cfg.get("baseline", 100.0))
    minimum = float(score_cfg.get("min", 0.0))
    return rules, baseline, minimum


ROOT_CAUSES_PATH = Path(__file__).resolve().parents[2] / "dq" / "config" / "root_causes.yml"


def load_root_causes(path: Path | None = None) -> dict[str, tuple[RootCause, ...]]:
    source = path or ROOT_CAUSES_PATH
    if not source.exists():
        return {}
    raw = yaml.safe_load(source.read_text()) or {}
    entries = raw.get("checks") if isinstance(raw, dict) else raw
    if not isinstance(entries, dict):
        return {}
    root_causes: dict[str, tuple[RootCause, ...]] = {}
    for check_name, payload in entries.items():
        if isinstance(payload, list):
            rows = payload
        elif isinstance(payload, dict):
            rows = [payload]
        else:
            continue
        parsed: list[RootCause] = []
        for row in rows:
            if not isinstance(row, dict):
                continue
            cause = row.get("probable_cause")
            fix = row.get("recommended_fix")
            if not cause or not fix:
                continue
            parsed.append(RootCause(probable_cause=str(cause), recommended_fix=str(fix)))
        if parsed:
            root_causes[check_name] = tuple(parsed)
    return root_causes
</file>

<file path="dq/validate/ge.py">
"""Great Expectations helpers for reporting."""

from __future__ import annotations

import json
from datetime import datetime
from pathlib import Path
from typing import Any, Iterable

from great_expectations import __version__ as ge_version
from great_expectations.core import ExpectationSuite
from great_expectations.core.batch import BatchDefinition, BatchMarkers
from great_expectations.core.expectation_configuration import ExpectationConfiguration
from great_expectations.core.expectation_validation_result import (
    ExpectationSuiteValidationResult,
    ExpectationValidationResult,
)
from great_expectations.core.id_dict import BatchSpec, IDDict
from great_expectations.core.run_identifier import RunIdentifier

from dq.validate.models import CheckResult
from dq.validate.paths import GE_ARTIFACTS_BASE


def build_expectation_suite(results: Iterable[CheckResult]) -> ExpectationSuite:
    expectations = [
        ExpectationConfiguration(
            expectation_type="dq_check",
            kwargs={
                "check_id": result.rule.id,
                "table": result.rule.table,
                "dimension": result.rule.dimension,
            },
        )
        for result in results
    ]
    return ExpectationSuite(expectation_suite_name="dq_checks", expectations=expectations)


def build_validation_result(
    suite: ExpectationSuite,
    results: Iterable[CheckResult],
    run_id: str,
    dataset_name: str,
    stage_path: Path,
    duckdb_path: Path,
    run_ts: datetime,
) -> ExpectationSuiteValidationResult:
    outcome: list[ExpectationValidationResult] = []
    for result in results:
        config = ExpectationConfiguration(
            expectation_type="dq_check",
            kwargs={
                "check_id": result.rule.id,
                "table": result.rule.table,
            },
        )
        outcome.append(
            ExpectationValidationResult(
                success=result.failure_count == 0,
                expectation_config=config,
                result={
                    "unexpected_count": result.failure_count,
                    "unexpected_percent": result.failure_rate * 100,
                },
                meta={
                    "issue_type": result.issue_type,
                    "samples": result.samples,
                },
            )
        )
    stats = {
        "evaluated_expectations": len(outcome),
        "success_percent": sum(1 for r in outcome if r.success) / len(outcome) * 100 if outcome else 100.0,
        "successful_expectations": sum(1 for r in outcome if r.success),
        "unsuccessful_expectations": sum(1 for r in outcome if not r.success),
    }
    run_identifier = RunIdentifier(run_name=run_id, run_time=run_ts)
    batch_definition = BatchDefinition(
        datasource_name="duckdb",
        data_connector_name="default_runtime_data_connector",
        data_asset_name=dataset_name,
        batch_identifiers=IDDict({}),
        batch_spec_passthrough={"path": str(stage_path)},
    )

    meta = {
        "active_batch_definition": batch_definition,
        "batch_markers": BatchMarkers({"ge_load_time": run_ts.isoformat()}),
        "batch_parameters": {"run_id": run_id},
        "batch_spec": BatchSpec({"path": str(duckdb_path)}),
        "checkpoint_id": None,
        "checkpoint_name": "dq_checkpoint",
        "expectation_suite_name": suite.name,
        "great_expectations_version": ge_version,
        "run_id": run_identifier,
        "validation_id": None,
        "validation_time": run_ts.isoformat(),
    }
    return ExpectationSuiteValidationResult(
        success=all(r.success for r in outcome),
        results=outcome,
        statistics=stats,
        meta=meta,
    )


def write_json(payload: dict[str, Any], target: Path) -> Path:
    target.parent.mkdir(parents=True, exist_ok=True)
    with target.open("w", encoding="utf-8") as handle:
        json.dump(payload, handle, ensure_ascii=False, indent=2)
    return target
</file>

<file path="dq/validate/models.py">
"""Data models for the validation runner."""

from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any

from dq.validate.config import CheckRule


@dataclass
class CheckResult:
    rule: CheckRule
    table: str
    stage_table: str
    failure_count: int
    total_rows: int
    failure_rate: float
    status: str
    penalty: float
    issue_type: str
    samples: list[dict[str, Any]]


@dataclass
class ValidationSummary:
    run_id: str
    run_ts: datetime
    dataset_name: str
    score: float
    subscores: dict[str, float]
    check_results_path: Path
    issue_log_path: Path
    expectation_suite_path: Path
    validation_result_path: Path
    run_history_path: Path
    issue_history_path: Path
    recurrence_summary_path: Path
</file>

<file path="README.md">
# DQSentry: Phase 0 — Design, Contracts, Scoring

## What this proves
- The team can codify a full data quality playbook (schema expectations, validation rules, scoring, alerts) without writing any ad-hoc scripts.
- The foundation is repeatable: every run is reproducible, generates a `run_id`, and outputs portable Parquet artifacts for auditing.
- Quality decisions are explainable: checks, thresholds, severities, and weights live in YAML so non-engineers can adjust them and anyone can understand how a score was derived.

## Project goals for Phase 0
1. **Establish single source-of-truth configs** for schema contracts (columns/types/keys) and validation rules grouped by soundness dimensions.
2. **Document the scoring model** so it can be explained in under two minutes and updated without touching Python.
3. **Plan the delivery surface**: GitHub Pages for the public scorecard and Streamlit Community Cloud for the CSV upload validator.

## Architecture highlights
- **Storage/engine:** DuckDB reads CSV exports, writes Parquet to `data/staging/`, and keeps audit marts in `data/marts/`.
- **Validation:** Great Expectations suites live under `dq/ge/` and are driven by the YAML rules in `dq/config/`.
- **Orchestration:** Python scripts in `scripts/` (ingest, profile, validate, score, publish, run_all) plus `Makefile` and GitHub Actions under `.github/workflows/` will be wired in later phases.
- **Publishing:** Static HTML scorecards land in `reports/latest/` and per-run artifacts in `reports/runs/` for historical tracking.

## Demo links (Phase 0 placeholders)
- Public scorecard: GitHub Pages (e.g., `https://<org>.github.io/DQSentry/latest-scorecard/`).
- Interactive validator: Streamlit Community Cloud app (CSV upload → score + downloads).

## Scoring model (explainable and stable)
1. Each validation check returns a `failure_rate` between 0 (clean) and 1 (all rows fail).
2. Each check defines:
   - A **severity** (1=notice, 5=critical).
   - A **weight** to reflect relative importance when computing penalties.
3. **Penalty per check**: `failure_rate * (severity / 5) * weight` so every penalty already sits in `[0, weight]` and scales with severity.
4. **Aggregate penalty**: sum all check penalties, normalize by the total possible weight (sum of all `weight` values across active checks).
5. **Overall score**: `max(0, 100 - 100 * normalized_penalty_sum)`; baseline is 100 when there are zero failures.
6. **Sub-scores**: repeat the same calculation within each dimension (completeness, validity, consistency, uniqueness, integrity) by summing only the checks that belong there.

## How to run (future work)
- `scripts/ingest.py` will copy `data/raw/*` into typed Parquet under `data/staging/`.
- `scripts/validate.py` will run Great Expectations suites backed by `dq/config/rules.yml` and write failure rates to `data/marts/`.
- `scripts/score.py` will apply the scoring formula above and emit human-friendly scorecards and issue logs in `reports/latest/` plus run-specific folders under `reports/runs/`.
- GitHub Actions `dq_push.yml` and `dq_scheduled.yml` will orchestrate CI and scheduled audits.

## Run expectations
- Never mutate files in `data/raw/`; all cleansed outputs land in `data/staging/`.
- Each run captures a `run_id` and timestamp, stored alongside generated artifacts.
- Detection is separate from remediation: the toolkit flags issues but explicit cleaning steps log fixes to `data/staging/` or a dedicated `marts/fixes/` table.

## Streamlit CSV validator
Use the new Streamlit app under `dq/app/app.py` when you want to upload CSV exports and immediately see the scorecard, issue log, and downloadable artifacts without running the full pipeline locally.

### Local run
1. `pip install -r requirements.txt`
2. `streamlit run dq/app/app.py`
3. Pick the built-in Phase 1 synthetic sample or upload your own CSV(s) / ZIP bundle containing `districts.csv`, `users.csv`, `resources.csv`, `events.csv`, and `newsletter.csv`.

### Deployment
- Point Streamlit Community Cloud to this repository and use `dq/app/app.py` as the entry file.
- The app stages uploads in an isolated temp directory, runs the ingest+validation helpers, and surfaces downloads so the Streamlit surface can stay responsive for strangers sharing their own CSVs.

### Sample dataset
The sample bundle sits at `dq/app/assets/sample_dataset.zip`; it mirrors the standard `data/raw/phase1/42` exports (including `run_metadata.json`) so you can demo the app without supplying your own files.
</file>

<file path="requirements.txt">
duckdb>=1.0,<1.4
pandas>=2.0,<2.3
pyarrow>=13,<18
jinja2>=3.1,<4.0
great_expectations>=0.18,<0.19
PyYAML>=6.0,<7
python-dateutil>=2.9,<3
streamlit>=1.30,<2.0
</file>

<file path="dq/validate/output.py">
"""Build output tables and persist results."""

from __future__ import annotations

import json
from datetime import datetime
from typing import Iterable

import pandas as pd

from dq.validate.models import CheckResult

from dq.validate.output_persistence import (
    append_issue_history,
    append_run_history,
    persist_dataframe,
    persist_recurrence_summary,
)
from dq.validate.output_recurrence import compute_recurrence_metrics


def build_check_results(
    results: Iterable[CheckResult], run_id: str, dataset_name: str
) -> pd.DataFrame:
    records: list[dict[str, object]] = []
    for result in results:
        records.append(
            {
                "run_id": run_id,
                "dataset_name": dataset_name,
                "table_name": result.table,
                "stage_table": result.stage_table,
                "check_id": result.rule.id,
                "dimension": result.rule.dimension,
                "description": result.rule.description,
                "rule_type": result.rule.rule_type,
                "columns": result.rule.columns or [],
                "column_regex": result.rule.column_regex,
                "severity": result.rule.severity,
                "weight": result.rule.weight,
                "threshold_warning": result.rule.threshold.warning,
                "threshold_fail": result.rule.threshold.fail,
                "failure_rate": result.failure_rate,
                "failure_count": result.failure_count,
                "total_rows": result.total_rows,
                "status": result.status,
                "penalty": result.penalty,
            }
        )
    return pd.DataFrame(records)


def build_issue_log(
    results: Iterable[CheckResult], run_id: str, dataset_name: str, run_ts: datetime
) -> pd.DataFrame:
    records: list[dict[str, object]] = []
    columns = [
        "run_id",
        "run_ts",
        "dataset_name",
        "table_name",
        "check_name",
        "dimension",
        "issue_type",
        "severity",
        "affected_rows",
        "affected_pct",
        "sample_bad_rows_json",
        "probable_root_cause",
        "recommended_fix",
        "root_cause_candidates",
    ]
    for result in results:
        if not result.failure_count:
            continue
        root_candidates = [
            {
                "probable_cause": rc.probable_cause,
                "recommended_fix": rc.recommended_fix,
            }
            for rc in result.rule.root_causes
        ]
        probable_root_cause = root_candidates[0]["probable_cause"] if root_candidates else result.rule.description
        recommended_fix = root_candidates[0]["recommended_fix"] if root_candidates else f"Enforce {result.rule.rule_type} for {result.rule.table}"
        records.append(
            {
                "run_id": run_id,
                "run_ts": run_ts.isoformat(),
                "dataset_name": dataset_name,
                "table_name": result.table,
                "check_name": result.rule.id,
                "dimension": result.rule.dimension,
                "issue_type": result.issue_type,
                "severity": result.rule.severity,
                "affected_rows": result.failure_count,
                "affected_pct": result.failure_rate,
                "sample_bad_rows_json": json.dumps(result.samples, ensure_ascii=False),
                "probable_root_cause": probable_root_cause,
                "recommended_fix": recommended_fix,
                "root_cause_candidates": json.dumps(root_candidates, ensure_ascii=False),
            }
        )
    return pd.DataFrame(records, columns=columns)


__all__ = [
    "append_issue_history",
    "append_run_history",
    "build_check_results",
    "build_issue_log",
    "compute_recurrence_metrics",
    "persist_dataframe",
    "persist_recurrence_summary",
]
</file>

<file path="dq/validate/runner.py">
"""Validation runner that combines DuckDB SQL with GE artifacts and reports."""

from __future__ import annotations

from datetime import datetime, timezone
from pathlib import Path
import duckdb

from dq.validate.ge import (
    build_expectation_suite,
    build_validation_result,
    write_json,
)
from dq.validate.metadata import collect_stage_metadata
from dq.validate.models import ValidationSummary
from dq.validate.output import (
    append_issue_history,
    append_run_history,
    build_check_results,
    build_issue_log,
    compute_recurrence_metrics,
    persist_dataframe,
    persist_recurrence_summary,
)
from dq.validate.paths import GE_ARTIFACTS_BASE
from dq.validate.rule_executor import RuleEvaluator
from dq.validate.scoring import calculate_scores
from dq.validate.config import load_rules

from dq.anomaly import run_anomaly_detection
from dq.schema_drift import run_schema_drift_detection

RULES_PATH = Path(__file__).resolve().parents[2] / "dq" / "config" / "rules.yml"


class ValidationRunner:
    def __init__(self, dataset_name: str, run_id: str, stage_path: Path, duckdb_path: Path) -> None:
        self.dataset_name = dataset_name
        self.run_id = run_id
        self.stage_path = stage_path
        self.duckdb_path = duckdb_path
        self.run_ts = datetime.now(timezone.utc)
        self.rules, self.baseline, self.score_min = load_rules(RULES_PATH)

    def run(self) -> ValidationSummary:
        with duckdb.connect(str(self.duckdb_path)) as con:
            metadata = collect_stage_metadata(con)
            evaluator = RuleEvaluator(metadata)
            results = [evaluator.evaluate(con, rule) for rule in self.rules]

        check_df = build_check_results(results, self.run_id, self.dataset_name)
        issue_df = build_issue_log(results, self.run_id, self.dataset_name, self.run_ts)
        score, subscores = calculate_scores(results, self.baseline, self.score_min)

        check_path = persist_dataframe(check_df, self.run_id, "dq_check_results")
        issue_path = persist_dataframe(issue_df, self.run_id, "dq_issue_log")
        run_history_path = append_run_history(
            self.run_id, self.run_ts, self.dataset_name, metadata
        )
        issue_history_path = append_issue_history(issue_df)
        recurrence_path = persist_recurrence_summary(
            compute_recurrence_metrics(), self.run_id
        )

        suite = build_expectation_suite(results)
        suite_path = write_json(
            suite.to_json_dict(),
            GE_ARTIFACTS_BASE / "expectations" / f"{suite.name}.json",
        )
        validation = build_validation_result(
            suite,
            results,
            self.run_id,
            self.dataset_name,
            self.stage_path,
            self.duckdb_path,
            self.run_ts,
        )
        validation_path = write_json(
            validation.to_json_dict(),
            GE_ARTIFACTS_BASE / "validations" / f"{self.run_id}--{suite.name}.json",
        )

        run_anomaly_detection(
            self.run_id,
            self.dataset_name,
            self.run_ts.isoformat(),
            self.duckdb_path,
        )

        run_schema_drift_detection(
            self.run_id,
            self.dataset_name,
            self.run_ts.isoformat(),
            self.stage_path,
            self.duckdb_path,
        )

        return ValidationSummary(
            run_id=self.run_id,
            run_ts=self.run_ts,
            dataset_name=self.dataset_name,
            score=score,
            subscores=subscores,
            check_results_path=check_path,
            issue_log_path=issue_path,
            expectation_suite_path=suite_path,
            validation_result_path=validation_path,
            run_history_path=run_history_path,
            issue_history_path=issue_history_path,
            recurrence_summary_path=recurrence_path,
        )
</file>

<file path="scripts/ingest_lib.py">
"""Reusable helpers for ingesting raw exports into DuckDB staging artifacts."""

from __future__ import annotations

import json
import shutil
from datetime import datetime, timezone
from pathlib import Path

import duckdb
import yaml
from dateutil import parser as date_parser

REPO_ROOT = Path(__file__).resolve().parents[1]
RAW_BASE = REPO_ROOT / "data" / "raw"
STAGING_BASE = REPO_ROOT / "data" / "staging"
MAPPINGS_FILE = REPO_ROOT / "dq" / "config" / "mappings.yml"

try:
    from scripts.ingest_tables import TABLE_SPECS
except ModuleNotFoundError:
    from ingest_tables import TABLE_SPECS


def sql_literal(value: str) -> str:
    return value.replace('"', '\\"').replace("'", "''")


def path_literal(path: Path) -> str:
    return path.as_posix().replace("'", "''")


def load_mappings() -> tuple[dict[str, str], dict[str, str], dict[str, str]]:
    with MAPPINGS_FILE.open() as handle:
        raw = yaml.safe_load(handle)
    state_codes = raw.get("state_codes") or {}
    grade_bands = raw.get("grade_bands") or {}
    district_overrides = raw.get("district_overrides") or {}

    state_map: dict[str, str] = {}
    for code, name in state_codes.items():
        state_map[code.strip().lower()] = code
        state_map[name.strip().lower()] = code
    state_map.setdefault("ca", "CA")
    state_map.setdefault("cax", "CA")
    state_map.setdefault("nyc", "NY")
    state_map.setdefault("texas", "TX")

    grade_map: dict[str, str] = {}
    for canonical, payload in grade_bands.items():
        canonical_key = canonical.strip()
        grade_map[canonical_key.lower()] = canonical_key
        for synonym in payload.get("synonyms", []):
            grade_map[synonym.strip().lower()] = canonical_key

    override_map = {
        key.strip().lower(): value.strip() for key, value in district_overrides.items()
    }
    return state_map, grade_map, override_map


def build_case_expression(
    column: str, mapping: dict[str, str], fallback: str
) -> str:
    clauses = []
    for alias, canonical in mapping.items():
        alias_literal = sql_literal(alias)
        canonical_literal = sql_literal(canonical)
        clauses.append(
            f"WHEN lower(trim({column})) = '{alias_literal}' THEN '{canonical_literal}'"
        )
    if not clauses:
        return fallback
    clause_text = " ".join(clauses)
    return f"(CASE {clause_text} ELSE {fallback} END)"


def parse_timestamp(value):
    if value is None:
        return None
    text = str(value).strip()
    if not text:
        return None
    try:
        parsed = date_parser.parse(text)
    except (ValueError, OverflowError, date_parser.ParserError):
        return None
    if parsed.tzinfo is not None:
        parsed = parsed.astimezone(timezone.utc).replace(tzinfo=None)
    return parsed


def ingest_table(con: duckdb.DuckDBPyConnection, sql: str) -> None:
    con.execute(sql)


def ingest_dataset(
    dataset_name: str,
    seed: int,
    force: bool = False,
    *,
    raw_path: Path | None = None,
    stage_path: Path | None = None,
    run_id: str | None = None,
) -> dict[str, str]:
    raw_path = raw_path or RAW_BASE / dataset_name / str(seed)
    if not raw_path.exists():
        raise SystemExit(f"Raw exports not found at {raw_path}")

    stage_path = stage_path or STAGING_BASE / dataset_name / str(seed)
    if stage_path.exists():
        if force:
            shutil.rmtree(stage_path)
        else:
            raise SystemExit(f"Staging path {stage_path} already exists. Use --force to rebuild.")
    stage_path.mkdir(parents=True, exist_ok=True)

    state_map, grade_map, district_overrides = load_mappings()
    state_case_expr = build_case_expression("state", state_map, "upper(trim(state))")
    grade_case_expr = build_case_expression("grade_band", grade_map, "trim(grade_band)")
    district_case_expr = build_case_expression("district_name", district_overrides, "trim(district_name)")

    parquet_path = stage_path / "parquet"
    parquet_path.mkdir(exist_ok=True)
    db_path = stage_path / "staging.duckdb"
    con = duckdb.connect(str(db_path))
    con.create_function(
        "py_parse_ts", parse_timestamp, return_type=duckdb.sqltype("TIMESTAMP")
    )

    for spec in TABLE_SPECS:
        source_path = path_literal(raw_path / spec["source"])
        sql = f"""
        CREATE TABLE staging_{spec['name']} AS
        {spec['select'].format(
            state_case_expr=state_case_expr,
            grade_case_expr=grade_case_expr,
            district_case_expr=district_case_expr,
            source_path=source_path,
        )}
        """
        ingest_table(con, sql)

    for table in (spec["name"] for spec in TABLE_SPECS):
        dest = parquet_path / f"{table}.parquet"
        con.execute(f"COPY staging_{table} TO '{dest.as_posix()}' (FORMAT PARQUET)")

    raw_metadata_path = raw_path / "run_metadata.json"
    raw_metadata: dict[str, object] = {}
    if raw_metadata_path.exists():
        raw_metadata_text = raw_metadata_path.read_text()
        raw_metadata = json.loads(raw_metadata_text)
        if run_id is None:
            run_id = raw_metadata.get("run_id")
        (stage_path / "run_metadata.json").write_text(raw_metadata_text)

    if run_id is None:
        generated_id = f"{dataset_name}-{seed}-{int(datetime.now(timezone.utc).timestamp())}"
        run_id = generated_id
        metadata_payload = {
            "dataset_name": dataset_name,
            "seed": seed,
            "run_id": run_id,
            "generated_at": datetime.now(timezone.utc).isoformat(),
        }
        (stage_path / "run_metadata.json").write_text(json.dumps(metadata_payload, indent=2))

    ingest_metadata = {
        "dataset_name": dataset_name,
        "seed": seed,
        "run_id": run_id,
        "ingested_at": datetime.now(timezone.utc).isoformat(),
        "raw_metadata": raw_metadata,
        "duckdb_path": str(db_path),
        "parquet_path": str(parquet_path),
    }
    (stage_path / "ingest_metadata.json").write_text(json.dumps(ingest_metadata, indent=2))
    con.close()

    return {
        "stage_path": str(stage_path),
        "db_path": str(db_path),
        "parquet_path": str(parquet_path),
        "run_id": str(run_id),
    }
</file>

</files>
